{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.othermod.betareg import BetaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import gamma\n",
    "from scipy.special import digamma\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классы стоперов, градиентов и регрессоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stoper:\n",
    "    \"\"\"\n",
    "    Базовый класс для любого стопера.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "                x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен, \n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "class StoperK(Stoper):\n",
    "    \"\"\"\n",
    "    Класс стопера, критерием которого является количество итераций.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_step: int = 100):\n",
    "        \"\"\"\n",
    "        Инициализирует класс итерирующего стопера.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            max_step (int): максимальное количество итераций.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_step = max_step\n",
    "        self.k = 0\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен,\n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        if self.k >= self.max_step:\n",
    "            return True\n",
    "\n",
    "        self.k += 1\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class StoperPoint(Stoper):\n",
    "    \"\"\"\n",
    "    Класс стопера, критерием которого является\n",
    "    норма разности нынешней и прошлой точки.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon: float = 1e-3, max_step: int = 1000000):\n",
    "        \"\"\"\n",
    "        Инициализирует класс итерирующего стопера.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            epsilon (float): условие на норму разности.\n",
    "\n",
    "            max_step (int): максимальное количество итераций.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_step = max_step\n",
    "        self.epsilon = epsilon\n",
    "        self.k = 0\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен,\n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        if self.k >= self.max_step:\n",
    "            return True\n",
    "\n",
    "        self.k += 1\n",
    "\n",
    "        if self.k == 1:\n",
    "            self.back_x = x\n",
    "        else:\n",
    "            norm = np.linalg.norm(x - self.back_x)\n",
    "            self.back_x = x\n",
    "\n",
    "            return not norm >= self.epsilon\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class Direction:\n",
    "    \"\"\"\n",
    "    Базовый класс для направления по которому идёт градиентный спуск.\n",
    "    \"\"\"\n",
    "    def __init__(self, size_param: int):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            size_param (int): размерность оптимизируемого значения.\n",
    "        \"\"\"\n",
    "        self.size_param = size_param\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        return np.zeros(self.size)\n",
    "\n",
    "\n",
    "def simple_derivative(x: np.array, X: np.array, y: np.array, size: int):\n",
    "    \"\"\"\n",
    "    Производная для метода наименьших квадратов.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти производную.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "\n",
    "        size (int): размер выборки.\n",
    "    \"\"\"\n",
    "    return 2 / size * X.T.dot(X.dot(x) - y)\n",
    "\n",
    "\n",
    "def log_likelyhood_simple(x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Функция правдоподобия для обычной линейной регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти функцию правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    errors = y - X.dot(x)\n",
    "    sigma = np.var(errors)\n",
    "    \n",
    "    return -np.sum(0.5 * np.log(2 * np.pi) + np.log(sigma) + errors ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "\n",
    "class DirectionData(Direction):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии.\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.array, y: np.array, derivative, alpha: float = 1e-3):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска. \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.derivative = derivative\n",
    "        self.alpha = alpha\n",
    "        self.size = y.shape[0]\n",
    "        self.size_param = X.shape[1]\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        return -self.alpha * self.derivative(x, self.X, self.y, self.size)\n",
    "\n",
    "\n",
    "class DirectionDataStochastic(DirectionData):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии со стохастическим методом.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.array,\n",
    "        y: np.array,\n",
    "        derivative,\n",
    "        alpha: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии\n",
    "        со стохастическим методом.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска. \n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "        \"\"\"\n",
    "        super().__init__(X, y, derivative, alpha)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        batch = random.sample(\n",
    "            range(self.X.shape[0]), min(self.batch_size, self.y.shape[0])\n",
    "        )\n",
    "        X_batch, y_batch = self.X[batch], self.y[batch]\n",
    "\n",
    "        return -self.alpha * self.derivative(x, X_batch, y_batch, self.batch_size)\n",
    "\n",
    "\n",
    "class DirectionDataRMSprop(DirectionDataStochastic):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии с методом RMSprop .\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.array,\n",
    "        y: np.array,\n",
    "        derivative,\n",
    "        alpha: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        eta: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии\n",
    "        с методом RMSprop.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска. \n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "\n",
    "            eta (float): параметр сглаживания.\n",
    "        \"\"\"\n",
    "        super().__init__(X, y, derivative, alpha, batch_size)\n",
    "        self.eta = eta\n",
    "        self.back_direct = []\n",
    "        self.G = []\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        batch = random.sample(\n",
    "            range(self.X.shape[0]), min(self.batch_size, self.y.shape[0])\n",
    "        )\n",
    "        X_batch, y_batch = self.X[batch], self.y[batch]\n",
    "\n",
    "        self.back_direct.append(\n",
    "            self.alpha * self.derivative(x, X_batch, y_batch, self.batch_size)\n",
    "        )\n",
    "        k = self.back_direct[-1].shape[0]\n",
    "        matrixF = np.zeros((k, k))\n",
    "\n",
    "        for deltaF_k in self.back_direct:\n",
    "            matrixF += deltaF_k.dot(deltaF_k.T)\n",
    "\n",
    "        if len(self.back_direct) == 1:\n",
    "            self.G.append(matrixF)\n",
    "        else:\n",
    "            self.G[0] = self.eta * self.G[0] + (1 - self.eta) * matrixF\n",
    "\n",
    "        return -self.back_direct[-1] * (np.diag(self.G[0]) + np.ones(k) * 1e-8) ** (\n",
    "            -1 / 2\n",
    "        )\n",
    "\n",
    "\n",
    "def logit_fun(x: np.array):\n",
    "    \"\"\"\n",
    "    Logit функция для перевода величины, которая принимает вещественные значения,\n",
    "    в величину, которая принимает значения от 0 до 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def log_likelyhood_beta(x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Функция правдоподобия для бета-регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти функцию правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    phi = x[0]\n",
    "    beta = x[1:]\n",
    "    mu = logit_fun(X.dot(beta))\n",
    "\n",
    "    log_gamma_phi = np.log(gamma(phi))\n",
    "    log_gamma_mu_phi = np.log(gamma(mu * phi))\n",
    "    log_gamma_minus_mu_phi = np.log(gamma((1 - mu) * phi))\n",
    "    \n",
    "    return np.sum(log_gamma_phi - log_gamma_mu_phi - log_gamma_minus_mu_phi + (mu * phi - 1) * np.log(y) + ((1 - mu) * phi - 1) * np.log(1 - y))\n",
    "\n",
    "\n",
    "def logbeta_derivative(x: np.array, X: np.array, y: np.array, size: int):\n",
    "    \"\"\"\n",
    "    Производная для бета-регрессии (производная функции правдоподобия).\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти производную.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "\n",
    "        size (int): размер выборки.\n",
    "    \"\"\"\n",
    "    phi = x[0]\n",
    "    beta = x[1:]\n",
    "    mu = logit_fun(X.dot(beta))\n",
    "\n",
    "    grad_mu = np.tile(np.array([mu * (1 - mu)]).transpose(), (1, X.shape[1])) * X\n",
    "\n",
    "    digamma_mu_phi = digamma(mu * phi)\n",
    "    digamma_1_minus_mu_phi = digamma((1 - mu) * phi)\n",
    "\n",
    "    dl_dbeta = np.sum(\n",
    "        np.tile(\n",
    "            np.array(\n",
    "                [\n",
    "                    phi\n",
    "                    * (\n",
    "                        digamma_1_minus_mu_phi\n",
    "                        - digamma_mu_phi\n",
    "                        + np.log(y)\n",
    "                        - np.log(1 - y)\n",
    "                    )\n",
    "                ]\n",
    "            ).transpose(),\n",
    "            (1, X.shape[1]),\n",
    "        )\n",
    "        * grad_mu,\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    dl_dphi = np.sum(\n",
    "        digamma(phi)\n",
    "        - mu * digamma_mu_phi\n",
    "        - (1 - mu) * digamma_1_minus_mu_phi\n",
    "        + mu * np.log(y)\n",
    "        + (1 - mu) * np.log(1 - y)\n",
    "    )\n",
    "\n",
    "    return -np.hstack((dl_dphi, dl_dbeta))\n",
    "\n",
    "\n",
    "def BIC(log_likelyhood, x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Байесовский информационный критерий.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        log_likelyhood (function): функция максимального правдоподобия.\n",
    "\n",
    "        x (np.array): точка, в которой считается функция максимального правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    X = scale(X)\n",
    "    X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "\n",
    "    return x.shape[0] * np.log(X.shape[0]) - 2 * log_likelyhood(x, X, y)\n",
    "\n",
    "\n",
    "class GradDesc:\n",
    "    \"\"\"\n",
    "    Класс градиентного спуска.\n",
    "    \"\"\"\n",
    "    def __init__(self, stoper: Stoper):\n",
    "        \"\"\"\n",
    "        Инициализирует класс градиентного спуска.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "\n",
    "            stoper (Stoper): класс для критерия остановки градиентного спуска.\n",
    "        \"\"\"\n",
    "        self.stoper = stoper\n",
    "\n",
    "    def optimise(\n",
    "        self, direction: Direction = Direction(1), start_point: np.array = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ищет точку минимума функции градиентным спуском.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "\n",
    "            direction (Direction): класс направления градиентного спуска.\n",
    "\n",
    "            start_point (np.array): начальная точка поика минимума (опционально).\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "\n",
    "            x_t (np.array): точка минимума функции.\n",
    "        \"\"\"\n",
    "        if start_point is None:\n",
    "            start_point = np.zeros(direction.size_param)\n",
    "\n",
    "        x_t = start_point\n",
    "\n",
    "        while not self.stoper.stop(x_t):\n",
    "            x_t = x_t + direction.get_direction(x_t)\n",
    "\n",
    "        return x_t\n",
    "\n",
    "\n",
    "class SGDLinearRegressor(RegressorMixin):\n",
    "    \"\"\"\n",
    "    Класс линейной регрессии.\n",
    "    \"\"\"\n",
    "    def __init__(self, stoper: Stoper, alpha: float = 0.01, batch_size: int = 64):\n",
    "        \"\"\"\n",
    "        Инициализирует класс линейной регрессии.\n",
    "\n",
    "        Параметры:\n",
    "            stoper (Stoper): класс критерия остановки градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "        \"\"\"\n",
    "        self.grad_desc = GradDesc(stoper)\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        \"\"\"\n",
    "        Тренирует модель линейной регрессии на данных и таргете.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            self (SGDLinearRegressor): сам класс линейной регрессии.\n",
    "        \"\"\"\n",
    "        X = self.scaler.fit(X).transform(X)\n",
    "        X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "\n",
    "        \n",
    "        self.W = self.grad_desc.optimise(\n",
    "            DirectionDataRMSprop(\n",
    "                X, np.array(y), simple_derivative, self.alpha, self.batch_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        \"\"\"\n",
    "        Предсказывает значение таргета на данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица данных.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            y (np.array): предсказанный таргет.\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        return np.hstack((X, np.ones((X.shape[0], 1)))).dot(self.W)\n",
    "\n",
    "\n",
    "class SGDBetaRegressor(RegressorMixin):\n",
    "    \"\"\"\n",
    "    Класс бета-регрессии.\n",
    "    \"\"\"\n",
    "    def __init__(self, stoper: Stoper, alpha: float = 0.01, batch_size: int = 64):\n",
    "        \"\"\"\n",
    "        Инициализирует класс бета-регрессии.\n",
    "\n",
    "        Параметры:\n",
    "            stoper (Stoper): класс критерия остановки градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "        \"\"\"\n",
    "        self.grad_desc = GradDesc(stoper)\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Тренирует модель бета-регрессии на данных и таргете.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            self (SGDLinearRegressor): сам класс бета-регрессии.\n",
    "        \"\"\"\n",
    "        X = self.scaler.fit(X).transform(X)\n",
    "        X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "        self.W = self.grad_desc.optimise(\n",
    "            DirectionDataRMSprop(\n",
    "                np.array(X),\n",
    "                np.array(y),\n",
    "                logbeta_derivative,\n",
    "                self.alpha,\n",
    "                self.batch_size,\n",
    "            ),\n",
    "            start_point=np.hstack((1, np.zeros(X.shape[1]))),\n",
    "        )\n",
    "        \n",
    "        self.phi = self.W[0]\n",
    "        self.W = self.W[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает значение таргета на данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица данных.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            y (np.array): предсказанный таргет.\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        return logit_fun(np.hstack((np.array(X), np.ones((X.shape[0], 1)))).dot(self.W))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ и результаты\n",
    "\n",
    "Для начала проверим обычный градиентный спуск на простой функции. Пусть это будет:\n",
    "$$\n",
    "y = x ^2 + 3 x + 2\n",
    "$$\n",
    "Точка минимума у такой функции является $x = -1.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DirectionPolynome:\n",
    "    def __init__(self, size_param = 1):\n",
    "        self.size_param = size_param\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        return -1e-2 * (2 * x + 3)\n",
    "\n",
    "\n",
    "GradDesc(StoperK(1000)).optimise(DirectionPolynome(), start_point=np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё верно.\n",
    "\n",
    "### Линейная регрессия\n",
    "\n",
    "Теперь проверим модификацию градиентного спуска (RMSprop) в задаче линейной регрессии. Для чего сгенирируем матрицу нормальных величин $\\mathbf{X} \\in \\mathbb{R} ^{n \\times m}$ и таргет $\\overline{y} \\in \\mathbb{R} ^{n}$, зависимый от матрицы $\\mathbf{X}$, где $n = 1000$, $m = 6$ (истинные коэффициенты можете видеть ниже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n",
      " [-0.23413696  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      " [-0.46341769 -0.46572975  0.24196227 -1.91328024 -1.72491783]\n",
      " ...\n",
      " [ 1.63579781 -0.22104246  0.0693698   0.19259692  2.39210964]\n",
      " [-2.09935626  0.68322301 -0.11480225  0.56677173 -0.65737255]\n",
      " [-0.04896503  0.71141058  3.1129102   0.80803619 -0.8480656 ]]\n",
      "[ 6.21667046e+00 -2.95063847e+00 -1.74296395e+01 -1.11041229e+01\n",
      " -9.03240689e+00 -9.53233497e+00 -3.93467247e+00 -8.26572944e+00\n",
      " -3.78756067e+00 -3.30697592e+00  3.80852081e+00  5.75849389e-01\n",
      " -8.66885556e+00 -5.27162055e-02  1.01518423e+01 -1.26246558e+00\n",
      " -8.44025969e+00 -1.98210700e+00 -1.19185970e+00 -2.77387780e+00\n",
      " -8.83390395e+00  2.78969662e+00  1.05037907e+01  4.88392388e+00\n",
      " -1.03885000e+01 -2.01196580e+00 -8.17472738e-01  2.37707414e+00\n",
      "  4.92307239e+00  8.63631858e-01 -1.31277880e-01 -5.15315186e+00\n",
      " -6.46318849e+00 -2.99990632e+00 -1.59837996e+00 -9.77875393e-01\n",
      "  2.99513528e+00 -6.48862716e+00 -6.74640451e+00 -3.14659319e+00\n",
      "  4.80491249e+00  1.32294277e-01  3.53140100e+00 -6.25030524e+00\n",
      " -1.30959480e+01 -2.72160702e+00 -7.68392650e+00 -9.33984412e+00\n",
      "  2.42932285e+00  7.89647837e+00  2.92072796e+00  4.00372994e+00\n",
      " -5.68514991e+00 -1.42517892e+00 -6.04324980e+00 -1.35787197e+00\n",
      " -8.22586859e+00 -3.32537559e+00  2.46144562e+00  3.44116196e+00\n",
      " -1.34588843e+00  5.53306685e+00  4.09191353e+00  2.60067026e-01\n",
      "  1.31017009e+01  2.03871633e+00 -7.62159856e+00  4.39383733e-01\n",
      " -5.78334364e+00 -7.34217077e+00 -1.74029916e+00 -1.28242437e+00\n",
      "  4.92042406e+00 -3.16684799e+00  4.17178794e+00  1.17910709e+01\n",
      " -5.25559388e+00  3.52436573e+00  1.10826800e+01 -6.08713936e+00\n",
      " -5.64514700e+00 -2.35798143e+00  5.84666679e+00  6.26492694e+00\n",
      "  1.07997273e+01  4.90033156e+00 -2.25336146e+00 -7.02051053e+00\n",
      " -4.55424559e-01 -3.77180140e+00  1.87777312e+00 -6.35872391e+00\n",
      "  5.93200413e-01 -3.40179419e+00  3.21592326e+00  1.70560657e+01\n",
      " -5.49521736e-01  5.63173916e+00 -4.60308469e+00 -8.94572076e+00\n",
      "  8.35497004e+00 -2.26439268e+00 -6.96115958e+00  1.63430855e+00\n",
      "  1.67478898e+00 -5.61935900e+00 -1.31099882e+01  2.62189600e+00\n",
      "  2.88070394e+00  6.87056612e-01 -2.24538883e+00  1.82526791e+00\n",
      " -6.62196494e-02 -1.82106833e+01 -1.55596079e-01 -1.25198039e+01\n",
      "  1.22272197e+01  2.31508051e+00 -2.63192184e+00 -3.80673495e+00\n",
      "  2.92031097e+00 -1.10857915e+01 -8.01360149e+00  5.05831292e-01\n",
      "  5.51007848e+00 -4.38566575e+00 -3.61400308e+00 -1.02243929e+01\n",
      " -7.68113298e+00 -8.69112009e+00 -3.06016689e+00  1.00242056e-01\n",
      " -6.71639487e+00 -1.90322985e+01 -8.79112955e+00 -7.05474235e-01\n",
      "  3.72053606e-02  3.73514844e+00  4.21995001e+00  4.42348538e+00\n",
      " -9.21221808e+00 -1.71558686e+01  1.97991766e+00 -3.42476165e+00\n",
      "  1.65445691e+00 -5.84326687e+00 -1.42001938e+01  8.08773942e+00\n",
      " -4.71675422e+00 -5.36235468e+00  3.66212734e+00 -4.35809662e+00\n",
      " -4.60517843e+00 -8.29122283e+00  2.10299334e+00 -5.04824652e+00\n",
      " -9.04381482e-01  2.28833596e+00 -9.59553948e+00 -2.65879978e+00\n",
      " -4.82091631e+00  3.80535857e+00 -3.11638641e+00 -3.01171731e+00\n",
      " -3.51357343e+00  1.50335476e+00 -9.29837411e+00  8.02469348e+00\n",
      "  2.00117061e+00 -8.49831313e+00  4.77343760e+00 -9.10391727e+00\n",
      "  3.80073212e+00  1.40872547e+00 -6.24005763e+00  5.94958019e-01\n",
      "  5.12257905e+00  3.39737736e+00  6.07388991e+00  5.17743093e+00\n",
      "  5.49995106e+00 -1.34396831e+00  7.11663046e+00 -2.94465886e+00\n",
      " -5.91414853e+00  2.40012785e+00 -1.07746423e+01 -3.74548682e+00\n",
      " -3.14463111e+00 -3.89857167e+00  7.71694655e+00  2.85761687e+00\n",
      "  5.88763058e+00 -4.97507284e+00 -9.97081440e+00 -1.20742211e+01\n",
      "  1.34578308e+00 -2.68354479e+00 -1.08667353e+01 -1.65619700e+00\n",
      " -2.81393614e+00  5.17528494e+00 -5.82754928e+00 -3.64512325e+00\n",
      "  1.31533887e+01  2.59380547e+00 -1.09570814e+00 -2.58295479e-01\n",
      " -3.62122878e+00  1.11002987e+01 -8.21864857e-01 -1.82355664e+00\n",
      " -1.87964617e+01 -9.17279808e+00 -8.20834072e+00  4.58510790e+00\n",
      "  1.01405008e+01 -1.86834709e+00 -2.85610856e+00  4.47771568e+00\n",
      " -7.60318324e+00 -3.91589655e+00  1.68479558e-01  3.82240949e+00\n",
      " -2.59148579e+00 -3.14632800e+00 -2.29401652e-01  6.78122504e+00\n",
      "  1.90565585e+00  7.53395379e+00 -1.22975365e+01  1.68038176e+00\n",
      " -9.14960026e+00  6.21199452e+00 -1.07680544e+00 -9.76264648e+00\n",
      " -1.10721154e+01  1.41281531e+00 -2.34059999e+00  4.39266654e-01\n",
      "  3.16567396e-01 -1.14336820e+01  2.12066777e+00 -6.09233989e+00\n",
      "  1.97531373e-01  4.53793121e+00  9.42851594e+00  5.89173714e+00\n",
      " -7.66855793e+00 -1.02596384e+00  3.63853128e+00 -5.06889079e+00\n",
      " -7.76841042e+00 -8.25759511e+00 -9.50304658e-01  2.31278756e+00\n",
      "  2.21243061e+00 -1.55163993e+01  6.80913708e+00  3.75299131e+00\n",
      " -3.63002327e+00  3.04577677e+00  1.84268711e+00 -8.43253837e+00\n",
      "  3.31597225e+00 -1.36068447e+01  1.22627812e+01 -6.08726296e+00\n",
      "  3.82260934e+00 -1.70716254e+01 -1.64280036e+00 -1.23058877e+01\n",
      " -7.65547468e+00 -3.50608533e+00 -7.02744649e+00 -6.88990546e+00\n",
      " -9.33735564e+00 -1.95550628e+00 -4.97510515e+00  3.54642097e+00\n",
      "  8.12143271e-01 -8.45402467e+00  1.38086196e+01  8.69601293e+00\n",
      " -2.68164964e+00  2.54526934e-01 -5.68709022e+00  7.07541681e-01\n",
      " -7.43743411e+00  4.05154413e+00  1.88259955e+01  5.76192104e+00\n",
      " -1.11514678e+01 -6.42641385e+00 -7.13513014e-01 -7.63846155e+00\n",
      " -6.00108696e+00 -1.01596701e+01  5.39505169e+00  9.52166686e+00\n",
      " -1.78578198e+00 -1.35472647e+00 -1.31783108e+01 -4.76098335e+00\n",
      " -5.51388425e+00 -8.72191744e-01 -6.87362078e+00 -1.07676439e+01\n",
      " -3.82648278e+00  1.56032279e-01 -6.93115480e+00  1.11062323e+01\n",
      "  3.62867997e+00 -1.20088005e+01 -1.26641233e+01 -1.13415787e+01\n",
      "  5.02348552e-01 -5.74721983e+00 -4.83037585e+00 -3.38226834e+00\n",
      " -3.50387361e+00 -3.66828897e+00 -5.73827158e+00  5.39095610e-01\n",
      " -8.93099614e+00 -2.44182928e+00  2.99714420e+00  7.53519929e+00\n",
      "  3.79940311e+00  3.22257828e+00 -3.51937194e+00 -9.22831579e+00\n",
      " -1.79883004e+00  2.88435363e+00 -8.40241199e+00  3.95309797e+00\n",
      "  3.38950381e-01 -1.12218671e+00 -1.16195615e+01  2.52413517e+00\n",
      "  4.42159204e+00 -3.45633906e+00  4.26101726e+00  1.94765469e+00\n",
      "  3.37285955e-01  1.63603608e+00 -4.75657840e+00 -6.42086435e-01\n",
      "  2.18203014e+00  1.15950319e+00 -1.29100717e+01  8.58948671e+00\n",
      " -8.46925764e+00  3.74145870e+00  2.46166622e+00 -6.39515298e+00\n",
      "  8.85244588e-01 -1.91157888e+00 -1.20002946e+00  3.59353968e+00\n",
      " -9.18121365e+00 -1.26631389e+01  1.56066440e+00 -8.96615833e+00\n",
      " -5.79235715e+00 -7.23311046e+00  1.12375281e+00 -4.59994967e+00\n",
      " -5.22737049e+00  7.90375723e+00  6.67088451e+00  1.04902812e+00\n",
      " -1.25022910e+01 -9.46101897e+00  2.27517500e+00 -1.11221451e+01\n",
      "  2.43965419e+00 -8.85991114e+00 -1.07991744e+00 -1.76650831e+00\n",
      " -1.54456185e+00 -1.13077071e+01  5.11304405e+00 -2.96335896e+00\n",
      "  1.48926704e+00 -4.77497390e-01 -2.62212060e-01  1.56839975e+00\n",
      " -5.94776075e+00  2.97676892e+00 -5.14416813e+00 -1.46964781e+00\n",
      "  2.13592515e+00 -2.83379962e+00 -8.19137361e+00 -4.79981505e+00\n",
      "  5.95750027e-01 -3.53915982e+00  1.58257418e+00 -9.91399347e-01\n",
      " -5.77541451e+00  2.87221509e+00 -9.22007833e+00  2.81283081e+00\n",
      " -1.23562881e+01  1.66266435e+00 -1.79697896e+00  2.51955435e+00\n",
      " -2.52707038e+00 -1.67151603e+00 -5.00369481e+00 -3.53621983e+00\n",
      " -3.11821906e+00  2.36441356e+00  1.63816886e+00 -2.61555399e+00\n",
      " -1.36779029e+01 -8.34892600e+00 -8.78609290e+00 -9.91923973e+00\n",
      "  3.87165736e+00 -5.77812280e+00 -7.77552132e+00 -1.74984502e+01\n",
      "  2.52883175e-01  2.34807538e-01  2.21342734e+00 -1.18139081e+01\n",
      " -1.28124721e+00 -5.76990231e+00  1.44630569e+00  4.19091877e+00\n",
      "  4.30544374e+00 -4.98858988e+00  5.03218048e-01  8.22017009e+00\n",
      "  4.79195291e+00 -4.26274137e+00 -1.24783956e+01 -5.59195285e+00\n",
      " -9.94573052e+00 -1.42676717e+00 -6.62787804e+00  9.44178753e+00\n",
      " -4.92410685e+00 -8.35794027e+00  4.50918457e+00 -7.54903373e-01\n",
      " -1.12820409e+01 -4.62806924e+00  1.45583578e+00 -1.48965439e+01\n",
      " -6.50811928e+00 -8.84196773e+00 -1.09629112e+00  4.95785429e+00\n",
      " -1.29300142e+01 -3.15548303e+00 -4.52373928e+00  1.20602728e+00\n",
      " -4.96890855e-01  6.63954181e+00  5.31610361e+00 -3.71123206e+00\n",
      "  7.61755210e+00  1.27915201e+00 -2.50081443e+00 -3.79536931e-01\n",
      " -5.08012972e+00  3.17027534e+00 -6.70632730e+00 -1.27737736e+01\n",
      "  1.90162991e+00 -6.65911823e+00 -2.36427293e-01 -7.10957229e-01\n",
      " -5.68722436e+00  3.33690730e+00 -8.61616890e+00 -1.92601684e+00\n",
      " -9.89473505e-01  8.86079550e-01 -2.09975930e+00  1.01687537e+00\n",
      " -1.48861230e+00  7.11160851e+00 -4.29553736e+00 -6.13273579e+00\n",
      "  7.08064696e+00 -6.20786991e+00 -9.01362729e+00  2.24906950e+00\n",
      " -8.46491640e+00  3.44262093e+00 -3.17755055e+00 -1.22928728e+01\n",
      " -6.05297332e+00 -2.94450874e+00  3.19028701e+00  4.97875720e+00\n",
      " -2.45753232e+00 -6.14187753e+00 -2.37445556e+00 -2.29727137e+00\n",
      "  1.68999295e+00  1.15578019e+01 -1.06231920e+01 -1.12515043e+01\n",
      " -6.35956042e+00  2.85557388e+00 -7.00403660e+00 -3.76751518e+00\n",
      " -1.57925539e-02  2.88222464e-01 -2.87872657e+00 -4.88036530e-01\n",
      "  7.37848040e+00 -9.25394143e+00 -2.27123841e+00 -1.21541220e-01\n",
      " -1.00730131e+01  3.91486962e+00 -2.52842793e+00 -3.16112325e-01\n",
      "  7.92812848e+00 -4.16599700e+00  2.52212446e+00 -7.55121818e+00\n",
      " -4.19782578e+00 -1.86582343e+00  6.89821306e+00 -3.77175040e+00\n",
      " -9.79858036e+00  3.27371914e+00 -5.48919787e+00  1.29540840e+00\n",
      "  1.84060171e+00  5.16105022e+00 -5.40479074e+00  7.99068134e+00\n",
      "  6.46273771e+00 -6.58577528e-01 -7.31515085e+00 -7.44893385e+00\n",
      "  9.11508721e+00 -7.58652109e+00 -6.08872042e-01  6.50654910e+00\n",
      "  1.39594005e+00  2.92170592e+00  1.46734566e+00 -7.49358405e+00\n",
      " -7.76060330e+00 -4.22046273e+00 -1.30248714e+01 -4.23341819e+00\n",
      " -7.54466350e+00 -4.22922962e+00 -7.70565683e+00 -1.20863723e+00\n",
      " -3.87839452e+00  2.19682900e+00 -7.12893289e+00  1.94101493e+00\n",
      " -8.95503912e+00  8.54871019e+00 -7.74495559e+00  1.38390625e+01\n",
      " -1.28831212e+01 -8.24497438e+00  1.90911202e+00  2.88498362e+00\n",
      "  3.40550646e+00  1.04764872e-01 -4.13045696e+00 -4.37529753e+00\n",
      " -6.61904135e+00 -6.94241839e+00 -6.79140046e+00 -5.58948903e+00\n",
      "  1.09801674e+00 -2.25161613e+00  5.30925336e+00  1.97120786e-01\n",
      " -2.24217960e-01  6.84544765e+00  4.19759349e+00 -1.41995284e+01\n",
      " -2.43655610e+00 -5.63314619e-02  8.58232340e+00  8.07618274e+00\n",
      " -1.02389990e+01 -1.14434421e+01  1.06407893e+01  3.05753055e+00\n",
      "  1.72154926e+00 -9.63322765e+00 -7.96841131e-01  2.25222096e+00\n",
      "  6.08991740e+00  5.43511707e+00 -6.92654340e+00 -2.73927241e+00\n",
      "  5.46660749e+00 -5.28481603e+00 -1.34053536e+01 -8.62564819e+00\n",
      "  1.96904388e+00  1.55427184e+00 -8.02451569e+00 -9.73734425e+00\n",
      " -4.84660839e+00  3.46561759e+00  8.99361235e+00 -4.08628631e-01\n",
      " -5.47893632e+00 -1.50116961e+01  4.44765218e-01  2.23184342e-02\n",
      "  3.23913542e+00 -4.43397944e+00 -1.15073377e+01  5.88424278e+00\n",
      " -4.67319079e+00 -6.18225153e+00  8.87666399e+00  5.08028135e+00\n",
      " -9.60672875e+00 -2.79920457e+00 -2.29042808e+00  2.71508719e+00\n",
      " -5.59811266e+00 -5.09580999e+00  3.16078202e+00  1.52899207e+00\n",
      "  9.92294869e-01  9.41997508e-01 -7.77281525e+00 -2.44254866e+00\n",
      "  6.98768159e+00 -7.42930000e-02  1.68727586e+00 -5.84318684e+00\n",
      "  2.11116224e+00 -3.00975354e+00  5.07813619e+00  4.41711038e+00\n",
      "  1.31662318e+01  2.14293860e+00  4.84378773e+00  4.73246927e+00\n",
      "  5.86374975e+00  1.27344136e+00 -1.66690981e+00  1.39954857e+00\n",
      "  5.15130393e+00 -4.84232993e+00 -3.78954961e+00 -6.02744037e+00\n",
      " -9.63297129e+00  9.40834396e+00 -1.05586065e+01  5.23955234e+00\n",
      " -6.71658926e-01 -8.67917855e+00 -1.20929798e+00 -1.65260260e+00\n",
      "  4.60741465e+00  2.83549202e+00 -8.47318138e-01  2.62135545e+00\n",
      " -1.37206342e+01  3.99028815e+00  3.10045522e+00  2.18306646e-01\n",
      "  9.00688921e+00  2.28215745e+00 -4.61116450e+00  9.14482278e+00\n",
      " -1.97799453e+00 -4.35689755e+00 -1.16369691e+01 -8.54093630e+00\n",
      " -1.77540535e+00 -3.13904310e-01  1.98613091e+00 -6.82687320e+00\n",
      "  5.24853887e+00 -5.17760302e+00 -9.03437051e+00  6.21763327e-01\n",
      "  9.85902411e+00 -8.13234001e+00 -7.17312148e+00  8.63810117e+00\n",
      " -8.23532862e+00 -6.89158737e+00 -6.41354817e+00  1.02113057e+00\n",
      " -1.98805066e+00 -4.13086512e-02 -1.45417843e+01  2.86813004e+00\n",
      "  3.73912562e+00 -1.90289407e+01 -2.51556934e+00  1.72125243e+00\n",
      " -6.47132623e+00 -6.56077132e+00  8.59044909e+00 -2.97894974e+00\n",
      "  7.22098698e+00 -5.49216236e-01  1.18357287e+00 -8.37655111e+00\n",
      " -5.99841624e+00  3.29010645e+00 -9.96997155e+00 -3.76376770e+00\n",
      " -6.50710403e+00 -1.19324564e+01  2.51573858e-01  3.88031806e+00\n",
      " -2.88696222e-01 -1.60420103e+01 -5.28632599e-02 -3.61712807e+00\n",
      " -2.18959868e+00  1.70593574e+00 -5.98911743e+00  2.13471281e+00\n",
      " -9.34610740e-01 -3.32724892e+00  6.24456752e+00  5.21351518e+00\n",
      "  4.05026331e+00  1.08739262e+01  4.97182674e+00  1.83969969e+00\n",
      " -1.83855292e+00 -2.53006676e+00 -1.88199158e+01 -5.66153529e-01\n",
      " -8.63395775e+00 -9.01956861e+00 -5.54300453e-02 -5.30402197e+00\n",
      " -1.30188009e+00  4.40396640e-01 -6.12283385e+00 -5.37975715e+00\n",
      "  5.51129298e+00 -2.77025686e+00 -6.49663787e+00 -1.74839241e+01\n",
      " -8.08480558e+00  1.13341894e-01 -4.45615452e+00 -8.27808912e+00\n",
      " -4.65343269e+00  5.24128372e+00 -2.07163455e+00 -8.58111616e+00\n",
      " -8.15236941e-01  3.24858357e+00 -1.24976581e+01 -6.28329320e+00\n",
      " -1.25555399e+01 -1.48007985e+00 -7.69757152e+00 -1.01010279e+01\n",
      " -4.73120926e+00 -3.44096236e+00 -1.07526818e+01 -9.70900605e+00\n",
      " -4.41249653e+00 -4.93663863e+00 -1.08787974e+01  4.46590264e+00\n",
      " -1.08887636e+01  4.34555351e+00 -9.04970003e+00  4.96292930e+00\n",
      " -9.23588504e+00 -7.31166327e+00  5.31622418e+00  1.83393821e+00\n",
      " -1.47761423e+01 -2.40850070e+00 -3.71406131e+00  1.69258419e+00\n",
      "  5.72797346e+00  1.34933267e+01 -4.60724647e+00  7.44282166e-01\n",
      " -4.07806492e+00  8.62251505e+00  1.74097343e+00  3.51223023e+00\n",
      " -2.22960314e-01 -9.16421012e+00 -3.04566076e+00 -5.55064396e+00\n",
      "  4.84954408e+00  6.34786371e+00 -2.24005750e+01 -4.89367107e-01\n",
      "  6.13304638e+00 -4.56707982e+00 -8.69463366e-01  7.82487983e+00\n",
      "  2.09192759e+00 -1.02124886e+01 -4.73838872e+00 -2.34324016e+00\n",
      " -1.63326621e+01  6.31232437e+00  3.05824872e-01 -2.28764868e+00\n",
      " -1.07689453e+00 -4.56591420e+00 -1.37762000e+01 -2.81186222e+00\n",
      " -3.09903157e+00 -1.22445658e+01  4.18023686e+00  2.54542491e+00\n",
      "  3.08912947e+00 -5.41265903e+00 -4.31189277e+00 -4.67997747e+00\n",
      " -1.30100468e+01  4.97067028e+00 -9.04567390e+00  8.52350226e+00\n",
      " -7.45146656e+00  2.69939426e+00 -1.88633067e+01 -1.72013480e+01\n",
      " -8.37817720e+00 -5.45904608e+00 -1.67661629e+01  2.03504247e+00\n",
      "  4.68320507e+00  2.95470183e+00  5.23638139e+00  7.73984602e+00\n",
      "  1.81037486e+00 -3.03548138e+00 -6.94931587e+00 -5.34794705e+00\n",
      " -3.49163870e+00 -6.35634174e+00 -1.02875744e+01 -4.42933209e+00\n",
      " -7.08642574e-01  5.82067981e+00 -2.05776073e+00 -1.25407371e+01\n",
      "  8.91050819e-01 -5.68275256e+00 -4.09874706e+00 -5.34490158e+00\n",
      " -1.07102217e+01  6.42095594e+00 -3.69943510e+00 -1.96443339e+01\n",
      "  2.67765720e+00 -6.68241465e+00 -1.42438024e+01  3.45505140e+00\n",
      "  3.73584221e+00 -5.86430695e+00 -1.36075446e+00 -7.58071459e+00\n",
      " -1.43618602e+00 -9.12862251e-01 -3.93146807e+00  2.88980415e+00\n",
      "  1.16079754e+00  3.90213433e+00 -1.59879907e+01 -3.18847677e+00\n",
      " -1.31548034e-01  5.54604998e-01 -1.27414320e+01 -1.41244107e+00\n",
      " -2.22821209e+00 -1.63506633e+00 -2.70774503e+00 -1.10873417e+01\n",
      "  7.44816391e+00 -2.63334701e+00  3.56585923e+00 -1.02089145e+01\n",
      " -3.87584282e-02  4.19335620e+00 -5.38766392e+00 -1.52943305e+01\n",
      " -1.73665464e+00  8.98175230e-01 -6.30117343e+00 -1.21088037e+00\n",
      "  1.10752432e+01 -5.26720470e+00 -5.66663182e+00 -5.88370973e+00\n",
      "  8.67687541e-01 -6.66425829e-02 -5.11596630e+00 -7.78778742e+00\n",
      "  4.07521319e+00  9.48648755e+00 -7.97015755e+00 -1.04384023e+01\n",
      "  1.69174567e+00 -4.63810580e+00 -1.51528644e+01 -1.86357986e+00\n",
      "  6.81872995e+00  5.94493047e-01 -9.82732365e+00  2.77049301e+00\n",
      "  4.87142197e+00  4.50339666e+00  4.85275441e-01 -1.54345637e+01\n",
      "  9.00320563e+00 -1.25004407e+01  1.94382052e+00  9.05111614e-01\n",
      " -3.02835065e+00 -1.25468258e+01 -9.50099390e+00  3.00281487e+00\n",
      " -8.13623133e+00 -1.23639436e+01  1.80085322e+00  3.98193950e-01\n",
      " -4.22934436e+00  7.66072289e+00  3.34403758e-01  4.38066153e+00\n",
      " -1.33583518e+00  6.13467844e+00  8.39995082e-02 -9.47169787e+00\n",
      "  8.12037862e-01 -7.59665552e+00 -3.63785265e+00 -9.73106088e+00\n",
      " -1.31395256e+00  3.36128270e+00 -4.62893086e+00 -9.13478228e+00\n",
      " -4.61468555e+00 -1.21833474e+01 -1.85579596e+00 -1.03688367e+01\n",
      " -5.35115739e+00  4.81701757e+00 -1.04276819e+01  3.57378591e+00\n",
      " -4.15649071e-01  2.96402208e+00 -9.55945027e-02  7.04761844e+00\n",
      " -3.50260159e+00 -9.11546517e+00  2.16562078e+00  1.87877217e+00\n",
      " -1.08812447e+01 -4.69526218e+00  6.70342307e+00 -1.05633600e+01\n",
      "  7.28923732e+00 -3.44838983e+00 -5.51473175e+00  4.26106877e-01\n",
      " -6.44469661e+00  2.07193267e+00  1.86282471e+00  9.59976645e+00\n",
      "  9.89657287e+00 -1.19923691e+01 -1.02124032e+00  2.83669830e+00\n",
      " -5.31647511e+00 -3.68254157e+00  6.40979074e+00 -4.69260651e+00\n",
      " -3.59575638e+00 -6.04599751e+00 -1.17183655e+01  1.11025000e+00\n",
      " -1.27171185e+00 -2.19846314e-01  1.05326103e+00 -3.15537322e-01]\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "w_true = np.array([1, 2, -1, 6, 0.3, -2])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.normal(0, 1, (n, 5))\n",
    "y = np.hstack((X, np.ones((n, 1)))).dot(w_true) + np.random.normal(0, 1, (n))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём эту выборку на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И применим наш класс линейной регрессии, проверив на тесте метрики RMSE, $R ^2 _{\\text{adj}}$ и BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98916325  2.05355263 -1.02132655  5.81031782  0.35006198 -2.19851591]\n",
      "1.0011673564263042\n",
      "0.9741400754147338\n",
      "859.1555817310243\n"
     ]
    }
   ],
   "source": [
    "reg = SGDLinearRegressor(StoperPoint(max_step=1000), alpha=1e-3, batch_size=64)\n",
    "reg = reg.fit(X_train, y_train)\n",
    "print(reg.W)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(root_mean_squared_error(y_test, y_pred))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(y_test, y_pred))\n",
    "    * (len(y_test) - 1)\n",
    "    / (len(y_test) - X_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, reg.W, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насколько метрики RMSE и BIC хороши трудно сказать, пока мы не сравнили с другим алгоритмом, однако поправленный $R ^2$ говорит о том, что модель хорошо соответсвует данным.\n",
    "\n",
    "Теперь применим к тем же данным линейную регрессию из пакета `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00677975  2.01830113 -1.0104927   6.02137558  0.34293177 -2.05958543]\n",
      "1.0045385531853415\n",
      "0.9739656277149537\n",
      "855.3975103368077\n"
     ]
    }
   ],
   "source": [
    "reg_lr = LinearRegression()\n",
    "reg_lr = reg_lr.fit(X_train, y_train)\n",
    "print(np.hstack((reg_lr.coef_, reg_lr.intercept_)))\n",
    "y_pred = reg_lr.predict(X_test)\n",
    "print(root_mean_squared_error(y_test, y_pred))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(y_test, y_pred))\n",
    "    * (len(y_test) - 1)\n",
    "    / (len(y_test) - X_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, np.hstack((reg_lr.coef_, reg_lr.intercept_)), X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты почти неотличимы от нашего подхода, на всё же лучше.\n",
    "\n",
    "### Бета-регрессия\n",
    "\n",
    "Теперь приступим к бета-регрессии, которая применяется для данных, когда таргет имеет значения от $0$ до $1$ не включительно.\n",
    "\n",
    "Сгенерируем данные для бета-регрессии для чего просто с помощью `logit` функции переведём таргет в нужный диапозон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_beta = logit_fun(np.hstack((scale(X), np.ones((n, 1)))).dot(w_true) + np.random.normal(0, 1, (n)))\n",
    "plt.hist(y_beta, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И применим к этим данным сначала линейную, потом бета-регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05000572  0.09768447 -0.06767869  0.33003329 -0.00940782  0.3922967 ]\n",
      "0.23672201630000234\n",
      "0.6992946245330802\n",
      "3698.0431622190604\n"
     ]
    }
   ],
   "source": [
    "X_beta_train, X_beta_test, y_beta_train, y_beta_test = train_test_split(\n",
    "    X, y_beta, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "reg_model_b = SGDLinearRegressor(StoperPoint(max_step=1000), alpha=1e-3, batch_size=64)\n",
    "reg_model_b = reg_model_b.fit(X_beta_train, y_beta_train)\n",
    "print(reg_model_b.W)\n",
    "y_pred_b = reg_model_b.predict(X_beta_test)\n",
    "print(root_mean_squared_error(y_beta_test, y_pred_b))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(y_beta_test, y_pred_b))\n",
    "    * (len(y_beta_test) - 1)\n",
    "    / (len(y_beta_test) - X_beta_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, reg_model_b.W, X_beta_test, y_beta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   2.  -1.   6.   0.3 -2. ]\n",
      "[ 0.40111935  0.87386957 -0.39197314  2.56884536  0.1154546  -0.85096191]\n",
      "0.04146668863245592\n",
      "-1428.684823179674\n"
     ]
    }
   ],
   "source": [
    "beta_reg_model = SGDBetaRegressor(StoperPoint(1e-3, max_step=1000), alpha=1e-3, batch_size=64)\n",
    "beta_reg_model = beta_reg_model.fit(X_beta_train, y_beta_train)\n",
    "print(w_true)\n",
    "print(beta_reg_model.W)\n",
    "y_beta_pred_model = beta_reg_model.predict(X_beta_test)\n",
    "print(root_mean_squared_error(y_beta_test, y_beta_pred_model, sample_weight=(1 + beta_reg_model.phi) / (y_beta_pred_model * (1 - y_beta_pred_model))))\n",
    "print(BIC(log_likelyhood_beta, np.hstack((beta_reg_model.phi, beta_reg_model.W)), X_beta_test, y_beta_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я взял данные по качеству сна студентов. В таблице представлены следующие признаки:\n",
    "\n",
    "- Age - возраст;\n",
    "- Gender - пол;\n",
    "- University_Year - год обучения;\n",
    "- Sleep_Duration - продолжительность сна;\n",
    "- Study_Hours - количество часов на учёбу за день;\n",
    "- Screen_Time - количество часов экранного времени за день;\n",
    "- Caffeine_Intake - среднее количество напитков с кофеином за день;\n",
    "- Physical_Activity - среднее количество минут на физическую активность за день;\n",
    "- Sleep_Quality\t- субъективная оценка сна от $1$ до $10$;\n",
    "- Weekday_Sleep_Start - время, когда студент ложится спать в будние дни;\n",
    "- Weekend_Sleep_Start - время, когда студент ложится спать в выходные дни;\n",
    "- Weekday_Sleep_End - время, когда студент встаёт в будние дни;\n",
    "- Weekend_Sleep_End - время, когда студент встаёт в выходные дни.\n",
    "\n",
    "Будем предсказывать качество сна, для чего преобразуем таргет в вектор от нуля до единицы не включительно. А так же пол преобразуем OneHot кодированием:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>University_Year</th>\n",
       "      <th>Sleep_Duration</th>\n",
       "      <th>Study_Hours</th>\n",
       "      <th>Screen_Time</th>\n",
       "      <th>Caffeine_Intake</th>\n",
       "      <th>Physical_Activity</th>\n",
       "      <th>Sleep_Quality</th>\n",
       "      <th>Weekday_Sleep_Start</th>\n",
       "      <th>Weekend_Sleep_Start</th>\n",
       "      <th>Weekday_Sleep_End</th>\n",
       "      <th>Weekend_Sleep_End</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Gender_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.95</td>\n",
       "      <td>14.16</td>\n",
       "      <td>4.05</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>0.15</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.10</td>\n",
       "      <td>8.21</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.45</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.47</td>\n",
       "      <td>6.88</td>\n",
       "      <td>10.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>19.82</td>\n",
       "      <td>4.08</td>\n",
       "      <td>6.69</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20.98</td>\n",
       "      <td>6.12</td>\n",
       "      <td>8.98</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  University_Year  Sleep_Duration  Study_Hours  Screen_Time  \\\n",
       "0   24                2             7.7          7.9          3.4   \n",
       "1   21                1             6.3          6.0          1.9   \n",
       "2   22                4             5.1          6.7          3.9   \n",
       "3   24                4             6.3          8.6          2.8   \n",
       "4   20                4             4.7          2.7          2.7   \n",
       "\n",
       "   Caffeine_Intake  Physical_Activity  Sleep_Quality  Weekday_Sleep_Start  \\\n",
       "0                2                 37           0.95                14.16   \n",
       "1                5                 74           0.15                 8.73   \n",
       "2                5                 53           0.45                20.00   \n",
       "3                4                 55           0.85                19.82   \n",
       "4                0                 85           0.25                20.98   \n",
       "\n",
       "   Weekend_Sleep_Start  Weekday_Sleep_End  Weekend_Sleep_End  Gender_Male  \\\n",
       "0                 4.05               7.41               7.06            0   \n",
       "1                 7.10               8.21              10.21            1   \n",
       "2                20.47               6.88              10.92            1   \n",
       "3                 4.08               6.69               9.42            0   \n",
       "4                 6.12               8.98               9.01            1   \n",
       "\n",
       "   Gender_Other  \n",
       "0             1  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_data = pd.read_csv(\"student_sleep_patterns.csv\", sep=\",\")\n",
    "beta_data = beta_data.drop(columns=\"Student_ID\")\n",
    "beta_data = pd.get_dummies(beta_data, columns=[\"Gender\"], drop_first=True, dtype=\"int8\")\n",
    "beta_data[\"University_Year\"] = [int(year[0]) for year in beta_data[\"University_Year\"]]\n",
    "target_column = \"Sleep_Quality\"\n",
    "features_columns = beta_data.drop(columns=target_column).columns\n",
    "\n",
    "beta_data[\"Sleep_Quality\"] = (\n",
    "    beta_data[\"Sleep_Quality\"] / max(beta_data[\"Sleep_Quality\"]) - 0.05\n",
    ")\n",
    "\n",
    "beta_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на расперделения таргета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(beta_data[target_column], bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Близко к равномерному, хотя вполне возможно, что слева есть небольшое увеличение количества.\n",
    "\n",
    "Разделим на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_data_train, beta_data_test, target_train, target_test = train_test_split(\n",
    "    beta_data[features_columns], beta_data[target_column], test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с линейной регрессии. Посмотрим, может, будет неплохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.22124512e-03 -1.20346004e-02  3.29574124e-04  1.79197048e-02\n",
      "  1.38279139e-02  6.95112305e-03 -1.85436606e-02 -6.44458647e-03\n",
      "  6.57673869e-03 -1.79171352e-02 -1.37556882e-02 -3.42013796e-02\n",
      "  9.52632497e-03  5.13662269e-01]\n",
      "0.31296537824645954\n",
      "-0.20932151446919\n",
      "1086.97736913916\n"
     ]
    }
   ],
   "source": [
    "reg_b = SGDLinearRegressor(StoperPoint(max_step=1000), alpha=1e-3, batch_size=64)\n",
    "reg_b = reg_b.fit(beta_data_train, target_train)\n",
    "print(reg_b.W)\n",
    "y_pred_b = reg_b.predict(beta_data_test)\n",
    "print(root_mean_squared_error(target_test, y_pred_b))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(target_test, y_pred_b))\n",
    "    * (len(target_test) - 1)\n",
    "    / (len(target_test) - beta_data_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, reg_b.W, beta_data_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по отрицательному значению поправленного $R ^2$ линейная модель здесь не очень хороша.\n",
    "\n",
    "Применим нашу бета-регрессию к данным и посмотрим на весовой RMSE и BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02195382  0.02745827  0.0374904   0.06578509 -0.02612991  0.04508176\n",
      "  0.05030977 -0.005901    0.0223599   0.00542515 -0.03026275 -0.00728989\n",
      "  0.06010478 -0.01304699]\n",
      "2.135825121906761\n",
      "0.30840566683208\n",
      "77.04684753646032\n"
     ]
    }
   ],
   "source": [
    "beta_reg = SGDBetaRegressor(StoperPoint(1e-5, max_step=1000), alpha=1e-3, batch_size=64)\n",
    "beta_reg = beta_reg.fit(beta_data_train, target_train)\n",
    "print(beta_reg.W)\n",
    "print(beta_reg.phi)\n",
    "y_pred = beta_reg.predict(beta_data_test)\n",
    "print(root_mean_squared_error(target_test, y_pred, sample_weight=(1 + beta_reg.phi) / (y_pred * (1 - y_pred))))\n",
    "print(BIC(log_likelyhood_beta, np.hstack((beta_reg.phi, beta_reg.W)), beta_data_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по RMSE результат не очень, так как можно сказать, что мы ошибаемся в среднем на $30 \\%$, что много. А по BIC мы сможем сравнить с пакетом `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              BetaModel Results                               \n",
      "==============================================================================\n",
      "Dep. Variable:          Sleep_Quality   Log-Likelihood:                 2.7516\n",
      "Model:                      BetaModel   AIC:                             24.50\n",
      "Method:            Maximum Likelihood   BIC:                             83.40\n",
      "Date:                Sat, 14 Dec 2024                                         \n",
      "Time:                        11:16:35                                         \n",
      "No. Observations:                 375                                         \n",
      "Df Residuals:                     360                                         \n",
      "Df Model:                          13                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0005      0.024     -0.021      0.983      -0.047       0.046\n",
      "x2             0.0241      0.052      0.460      0.646      -0.079       0.127\n",
      "x3            -0.0050      0.039     -0.128      0.898      -0.081       0.071\n",
      "x4             0.0199      0.016      1.246      0.213      -0.011       0.051\n",
      "x5             0.0072      0.066      0.110      0.913      -0.122       0.137\n",
      "x6             0.0205      0.034      0.608      0.543      -0.046       0.087\n",
      "x7            -0.0003      0.002     -0.182      0.856      -0.003       0.003\n",
      "x8            -0.0007      0.009     -0.072      0.943      -0.019       0.018\n",
      "x9            -0.0007      0.010     -0.075      0.940      -0.020       0.019\n",
      "x10           -0.0061      0.047     -0.129      0.898      -0.099       0.087\n",
      "x11           -0.0406      0.052     -0.786      0.432      -0.142       0.061\n",
      "x12           -0.0726      0.135     -0.537      0.591      -0.338       0.192\n",
      "x13            0.1136      0.146      0.780      0.435      -0.172       0.399\n",
      "const          0.2164      0.882      0.245      0.806      -1.512       1.944\n",
      "precision      0.7569      0.062     12.250      0.000       0.636       0.878\n",
      "==============================================================================\n",
      "0.3074667597019916\n",
      "90.3565616335168\n"
     ]
    }
   ],
   "source": [
    "beta_reg_mod = BetaModel(target_train, np.hstack((np.array(beta_data_train), np.ones((beta_data_train.shape[0], 1)))))\n",
    "beta_reg_mod = beta_reg_mod.fit()\n",
    "print(beta_reg_mod.summary())\n",
    "\n",
    "y_pred_mod = beta_reg_mod.predict(np.hstack((np.array(beta_data_test), np.ones((beta_data_test.shape[0], 1)))))\n",
    "print(root_mean_squared_error(target_test, y_pred_mod, sample_weight=(1 + beta_reg_mod.params[\"precision\"]) / (y_pred_mod * (1 - y_pred_mod))))\n",
    "print(BIC(log_likelyhood_beta, np.hstack((np.exp(beta_reg_mod.params[\"precision\"]), beta_reg_mod.params[:-1])), beta_data_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения RMSE алгоритмы примерно равны, но вот по BIC наша модель показала себя даже лучше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
