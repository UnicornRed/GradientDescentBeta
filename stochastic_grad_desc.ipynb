{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.othermod.betareg import BetaModel\n",
    "from statsmodels.genmod.families.links import Identity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import gamma\n",
    "from scipy.special import digamma\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классы стоперов, градиентов и регрессоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stoper:\n",
    "    \"\"\"\n",
    "    Базовый класс для любого стопера.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "                x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен,\n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "class StoperK(Stoper):\n",
    "    \"\"\"\n",
    "    Класс стопера, критерием которого является количество итераций.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_step: int = 100):\n",
    "        \"\"\"\n",
    "        Инициализирует класс итерирующего стопера.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            max_step (int): максимальное количество итераций.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_step = max_step\n",
    "        self.k = 0\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен,\n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        if self.k >= self.max_step:\n",
    "            return True\n",
    "\n",
    "        self.k += 1\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class StoperPoint(Stoper):\n",
    "    \"\"\"\n",
    "    Класс стопера, критерием которого является\n",
    "    норма разности нынешней и прошлой точки.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-3, max_step: int = 1000000):\n",
    "        \"\"\"\n",
    "        Инициализирует класс итерирующего стопера.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            epsilon (float): условие на норму разности.\n",
    "\n",
    "            max_step (int): максимальное количество итераций.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_step = max_step\n",
    "        self.epsilon = epsilon\n",
    "        self.k = 0\n",
    "\n",
    "    def stop(self, x: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Функция которая возвращает True, если выполнено условие для остановки,\n",
    "        иначе False.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            x (np.array): точка, в которой требуется проверить критерий.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            bool_stop (bool): True, если критерий остановки выполнен,\n",
    "            False - иначе.\n",
    "        \"\"\"\n",
    "        if self.k >= self.max_step:\n",
    "            return True\n",
    "\n",
    "        self.k += 1\n",
    "\n",
    "        if self.k == 1:\n",
    "            self.back_x = x\n",
    "        else:\n",
    "            norm = np.linalg.norm(x - self.back_x)\n",
    "            self.back_x = x\n",
    "\n",
    "            return not norm >= self.epsilon\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class Direction:\n",
    "    \"\"\"\n",
    "    Базовый класс для направления по которому идёт градиентный спуск.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size_param: int):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            size_param (int): размерность оптимизируемого значения.\n",
    "        \"\"\"\n",
    "        self.size_param = size_param\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        return np.zeros(self.size)\n",
    "\n",
    "\n",
    "def simple_derivative(x: np.array, X: np.array, y: np.array, size: int):\n",
    "    \"\"\"\n",
    "    Производная для метода наименьших квадратов.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти производную.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "\n",
    "        size (int): размер выборки.\n",
    "    \"\"\"\n",
    "    return 2 / size * X.T.dot(X.dot(x) - y)\n",
    "\n",
    "\n",
    "def log_likelyhood_simple(x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Функция правдоподобия для обычной линейной регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти функцию правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    errors = y - X.dot(x)\n",
    "    sigma = np.var(errors)\n",
    "\n",
    "    return -np.sum(0.5 * np.log(2 * np.pi) + np.log(sigma) + errors**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "class DirectionData(Direction):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.array, y: np.array, derivative, alpha: float = 1e-3):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.derivative = derivative\n",
    "        self.alpha = alpha\n",
    "        self.size = y.shape[0]\n",
    "        self.size_param = X.shape[1]\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        return -self.alpha * self.derivative(x, self.X, self.y, self.size)\n",
    "\n",
    "\n",
    "class DirectionDataStochastic(DirectionData):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии со стохастическим методом.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.array,\n",
    "        y: np.array,\n",
    "        derivative,\n",
    "        alpha: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии\n",
    "        со стохастическим методом.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "        \"\"\"\n",
    "        super().__init__(X, y, derivative, alpha)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        batch = random.sample(\n",
    "            range(self.X.shape[0]), min(self.batch_size, self.y.shape[0])\n",
    "        )\n",
    "        X_batch, y_batch = self.X[batch], self.y[batch]\n",
    "\n",
    "        return -self.alpha * self.derivative(x, X_batch, y_batch, self.batch_size)\n",
    "\n",
    "\n",
    "class DirectionDataRMSprop(DirectionDataStochastic):\n",
    "    \"\"\"\n",
    "    Класс направления для задачи линейной регрессии с методом RMSprop .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.array,\n",
    "        y: np.array,\n",
    "        derivative,\n",
    "        alpha: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        eta: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализирует класс направления для задачи линейной регрессии\n",
    "        с методом RMSprop.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "            derivative (function): функция которая по матрице данных, таргету и точке\n",
    "            возвращает направление для градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "\n",
    "            eta (float): параметр сглаживания.\n",
    "        \"\"\"\n",
    "        super().__init__(X, y, derivative, alpha, batch_size)\n",
    "        self.eta = eta\n",
    "        self.back_direct = []\n",
    "        self.G = []\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Возвращает направление для градиентного спуска.\n",
    "\n",
    "        Параметры:направление\n",
    "        ----------\n",
    "            x (np.array): точка (вектор), в которой требуется найти направление.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            direction (np.array): вектор направления.\n",
    "        \"\"\"\n",
    "        batch = random.sample(\n",
    "            range(self.X.shape[0]), min(self.batch_size, self.y.shape[0])\n",
    "        )\n",
    "        X_batch, y_batch = self.X[batch], self.y[batch]\n",
    "\n",
    "        self.back_direct.append(\n",
    "            self.alpha * self.derivative(x, X_batch, y_batch, self.batch_size)\n",
    "        )\n",
    "        k = self.back_direct[-1].shape[0]\n",
    "        matrixF = np.zeros((k, k))\n",
    "\n",
    "        for deltaF_k in self.back_direct:\n",
    "            matrixF += deltaF_k.dot(deltaF_k.T)\n",
    "\n",
    "        if len(self.back_direct) == 1:\n",
    "            self.G.append(matrixF)\n",
    "        else:\n",
    "            self.G[0] = self.eta * self.G[0] + (1 - self.eta) * matrixF\n",
    "\n",
    "        return -self.back_direct[-1] / np.sqrt(np.diag(self.G[0]) + np.ones(k) * 1e-8)\n",
    "\n",
    "\n",
    "def logit_fun(x: np.array):\n",
    "    \"\"\"\n",
    "    Logit функция для перевода величины, которая принимает вещественные значения,\n",
    "    в величину, которая принимает значения от 0 до 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def log_likelyhood_beta(x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Функция правдоподобия для бета-регрессии.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти функцию правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    phi = x[0]\n",
    "    beta = x[1:]\n",
    "    mu = logit_fun(X.dot(beta))\n",
    "\n",
    "    log_gamma_phi = np.log(gamma(phi))\n",
    "    log_gamma_mu_phi = np.log(gamma(mu * phi))\n",
    "    log_gamma_minus_mu_phi = np.log(gamma((1 - mu) * phi))\n",
    "\n",
    "    return np.sum(\n",
    "        log_gamma_phi\n",
    "        - log_gamma_mu_phi\n",
    "        - log_gamma_minus_mu_phi\n",
    "        + (mu * phi - 1) * np.log(y)\n",
    "        + ((1 - mu) * phi - 1) * np.log(1 - y)\n",
    "    )\n",
    "\n",
    "\n",
    "def logbeta_derivative(x: np.array, X: np.array, y: np.array, size: int):\n",
    "    \"\"\"\n",
    "    Производная для бета-регрессии (производная функции правдоподобия).\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        x (np.array): точка (вектор), в которой требуется найти производную.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "\n",
    "        size (int): размер выборки.\n",
    "    \"\"\"\n",
    "    phi = x[0]\n",
    "    beta = x[1:]\n",
    "    mu = logit_fun(X.dot(beta))\n",
    "\n",
    "    grad_mu = X.T.dot(np.diag(mu * (1 - mu)))\n",
    "\n",
    "    digamma_mu_phi = digamma(mu * phi)\n",
    "    digamma_1_minus_mu_phi = digamma((1 - mu) * phi)\n",
    "\n",
    "    dl_dbeta = phi * grad_mu.dot(digamma_1_minus_mu_phi - digamma_mu_phi + np.log(y) - np.log(1 - y))\n",
    "\n",
    "    dl_dphi = np.sum(\n",
    "        digamma(phi)\n",
    "        - mu * digamma_mu_phi\n",
    "        - (1 - mu) * digamma_1_minus_mu_phi\n",
    "        + mu * np.log(y)\n",
    "        + (1 - mu) * np.log(1 - y)\n",
    "    )\n",
    "\n",
    "    return -np.hstack((dl_dphi, dl_dbeta))\n",
    "\n",
    "\n",
    "def BIC(log_likelyhood, x: np.array, X: np.array, y: np.array):\n",
    "    \"\"\"\n",
    "    Байесовский информационный критерий.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "        log_likelyhood (function): функция максимального правдоподобия.\n",
    "\n",
    "        x (np.array): точка, в которой считается функция максимального правдоподобия.\n",
    "\n",
    "        X (np.array): матрица регрессоров.\n",
    "\n",
    "        y (np.array): вектор таргетов.\n",
    "    \"\"\"\n",
    "    X = scale(X)\n",
    "    X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "\n",
    "    return x.shape[0] * np.log(X.shape[0]) - 2 * log_likelyhood(x, X, y)\n",
    "\n",
    "\n",
    "class GradDesc:\n",
    "    \"\"\"\n",
    "    Класс градиентного спуска.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stoper: Stoper):\n",
    "        \"\"\"\n",
    "        Инициализирует класс градиентного спуска.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "\n",
    "            stoper (Stoper): класс для критерия остановки градиентного спуска.\n",
    "        \"\"\"\n",
    "        self.stoper = stoper\n",
    "\n",
    "    def optimise(\n",
    "        self, direction: Direction = Direction(1), start_point: np.array = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ищет точку минимума функции градиентным спуском.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "\n",
    "            direction (Direction): класс направления градиентного спуска.\n",
    "\n",
    "            start_point (np.array): начальная точка поика минимума (опционально).\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "\n",
    "            x_t (np.array): точка минимума функции.\n",
    "        \"\"\"\n",
    "        if start_point is None:\n",
    "            start_point = np.zeros(direction.size_param)\n",
    "\n",
    "        x_t = start_point\n",
    "\n",
    "        while not self.stoper.stop(x_t):\n",
    "            x_t = x_t + direction.get_direction(x_t)\n",
    "\n",
    "        return x_t\n",
    "\n",
    "\n",
    "class SGDLinearRegressor(RegressorMixin):\n",
    "    \"\"\"\n",
    "    Класс линейной регрессии.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stoper: Stoper, alpha: float = 0.01, batch_size: int = 64, direction: Direction = DirectionDataRMSprop):\n",
    "        \"\"\"\n",
    "        Инициализирует класс линейной регрессии.\n",
    "\n",
    "        Параметры:\n",
    "            stoper (Stoper): класс критерия остановки градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "\n",
    "            direction (Direction): направление градиента.\n",
    "        \"\"\"\n",
    "        self.grad_desc = GradDesc(stoper)\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.direction = direction\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        \"\"\"\n",
    "        Тренирует модель линейной регрессии на данных и таргете.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            self (SGDLinearRegressor): сам класс линейной регрессии.\n",
    "        \"\"\"\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "\n",
    "        self.W = self.grad_desc.optimise(\n",
    "            self.direction(\n",
    "                X, np.array(y), simple_derivative, self.alpha, self.batch_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        \"\"\"\n",
    "        Предсказывает значение таргета на данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица данных.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            y (np.array): предсказанный таргет.\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        return np.hstack((X, np.ones((X.shape[0], 1)))).dot(self.W)\n",
    "\n",
    "\n",
    "class SGDBetaRegressor(RegressorMixin):\n",
    "    \"\"\"\n",
    "    Класс бета-регрессии.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stoper: Stoper, alpha: float = 0.01, batch_size: int = 64, direction: Direction = DirectionDataRMSprop):\n",
    "        \"\"\"\n",
    "        Инициализирует класс бета-регрессии.\n",
    "\n",
    "        Параметры:\n",
    "            stoper (Stoper): класс критерия остановки градиентного спуска.\n",
    "\n",
    "            alpha (float): шаг градиентного спуска.\n",
    "\n",
    "            batch_size (int): размер батча для взятия подвыборки.\n",
    "\n",
    "            direction (Direction): направление градиента.\n",
    "        \"\"\"\n",
    "        self.grad_desc = GradDesc(stoper)\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.direction = direction\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Тренирует модель бета-регрессии на данных и таргете.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица регрессоров.\n",
    "\n",
    "            y (np.array): вектор таргетов.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            self (SGDLinearRegressor): сам класс бета-регрессии.\n",
    "        \"\"\"\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        X = np.hstack((np.array(X), np.ones((X.shape[0], 1))))\n",
    "        self.W = self.grad_desc.optimise(\n",
    "            self.direction(\n",
    "                np.array(X),\n",
    "                np.array(y),\n",
    "                logbeta_derivative,\n",
    "                self.alpha,\n",
    "                self.batch_size,\n",
    "            ),\n",
    "            start_point=np.hstack(((y.mean() * (1 - y.mean())) / y.var() - 1, np.random.normal(0, 1, X.shape[1]))),\n",
    "        )\n",
    "\n",
    "        self.phi = self.W[0]\n",
    "        self.W = self.W[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказывает значение таргета на данных.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "            X (np.array): матрица данных.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "            y (np.array): предсказанный таргет.\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        return logit_fun(np.hstack((np.array(X), np.ones((X.shape[0], 1)))).dot(self.W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ и результаты\n",
    "\n",
    "Для начала проверим обычный градиентный спуск на простой функции. Пусть это будет:\n",
    "$$\n",
    "y = x ^2 + 3 x + 2\n",
    "$$\n",
    "Точка минимума у такой функции является $x = -1.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5])"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DirectionPolynome:\n",
    "    def __init__(self, size_param=1):\n",
    "        self.size_param = size_param\n",
    "\n",
    "    def get_direction(self, x: np.array):\n",
    "        return -1e-2 * (2 * x + 3)\n",
    "\n",
    "\n",
    "GradDesc(StoperK(1000)).optimise(DirectionPolynome(), start_point=np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё верно.\n",
    "\n",
    "### Линейная регрессия\n",
    "\n",
    "Теперь проверим модификацию градиентного спуска (RMSprop) в задаче линейной регрессии. Для чего сгенирируем матрицу нормальных величин $\\mathbf{X} \\in \\mathbb{R} ^{n \\times m}$ и таргет $\\overline{y} \\in \\mathbb{R} ^{n}$, зависимый от матрицы $\\mathbf{X}$, где $n = 1000$, $m = 6$ (истинные коэффициенты можете видеть ниже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n",
      " [-0.23413696  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      " [-0.46341769 -0.46572975  0.24196227 -1.91328024 -1.72491783]\n",
      " ...\n",
      " [ 1.63579781 -0.22104246  0.0693698   0.19259692  2.39210964]\n",
      " [-2.09935626  0.68322301 -0.11480225  0.56677173 -0.65737255]\n",
      " [-0.04896503  0.71141058  3.1129102   0.80803619 -0.8480656 ]]\n",
      "[  6.64043014  -2.49722436 -15.63399629 -10.77403267  -9.76523597\n",
      "  -8.25810285  -4.98315512  -8.75350426  -3.05332735  -3.16544629\n",
      "   2.210199    -0.15769842  -8.67763921   0.18496711  10.07523318\n",
      "  -0.75070202  -6.33551018  -4.02428491  -1.3471847   -3.17202507\n",
      "  -8.7759962    3.52481342  10.68971521   6.48331864 -12.66558652\n",
      "  -0.7774538    0.21449355   2.81733335   3.63499667   0.84912286\n",
      "   1.10485792  -6.40749022  -7.19418911  -3.53834128  -2.3141142\n",
      "  -3.37223729   0.81004039  -5.80191182  -8.3702895   -3.52976138\n",
      "   4.30581667   1.82456511   3.70178676  -5.43748165 -13.9240068\n",
      "  -3.79268752  -5.60944015  -9.31780925   1.30746134   8.08315733\n",
      "   2.19166432   3.65256448  -6.02681014  -1.15386394  -4.82666899\n",
      "  -0.23634183  -8.9352211   -3.34483385   2.59995787   3.865052\n",
      "  -1.04069958   4.48615506   4.42715761   1.60113174  12.35804914\n",
      "   2.243593    -7.65299442  -1.16819457  -5.76278309  -7.63135444\n",
      "  -0.56137567  -1.06275492   4.31089802  -1.61147425   3.97094284\n",
      "  11.69027982  -5.29688134   2.48720683  10.74824151  -7.56521126\n",
      "  -4.65155748  -2.493906     5.7622       7.33418143   9.84343225\n",
      "   2.57565825  -2.23635538  -7.52553162  -0.53994303  -3.05821673\n",
      "   3.6985334   -5.65295043   1.98515879  -5.10589627   3.33103295\n",
      "  17.40198506  -1.43867556   6.75349026  -3.83622066  -9.01379303\n",
      "   7.32620105  -2.76607773  -7.20948472   1.55619082   1.27455313\n",
      "  -5.8210119  -12.0800159    2.57581678   3.9543926    0.60597407\n",
      "  -1.76474089  -0.71411372  -0.02632527 -16.8064437   -0.02141587\n",
      " -13.03916265  10.55385574   2.90453291  -1.04949387  -4.62357197\n",
      "   4.30090967 -10.64963608  -7.755981     0.57546474   4.82091403\n",
      "  -3.70203147  -3.34027382 -10.2101225   -7.83073094  -8.78894726\n",
      "  -3.49617267  -0.74359745  -7.10902898 -19.48588399  -7.20890638\n",
      "  -0.75570193  -0.44230219   3.00620724   4.91230079   5.08252466\n",
      "  -8.63811741 -17.69228247   0.94603661  -3.01730154   0.7464946\n",
      "  -5.96674668 -14.07590799   8.29912365  -5.46336558  -5.6879325\n",
      "   1.38630191  -3.50587197  -5.12328014  -7.76202182   2.97228033\n",
      "  -5.03737938  -1.04173396  -0.62983759  -9.41139366  -1.52862105\n",
      "  -5.09392703   5.8799986   -3.02442962  -3.02774259  -4.30817381\n",
      "   0.52973251  -8.79480449   8.1753075    2.76729144  -7.37025058\n",
      "   4.46354282  -7.83033172   4.21535902   2.40389943  -6.78974082\n",
      "  -1.20647208   6.42416703   5.25039585   6.19877679   5.16173314\n",
      "   5.28129176  -0.8976478    7.31726976  -3.31216927  -6.95457942\n",
      "   4.35517662 -10.40570289  -3.95929561  -2.226655    -5.6297413\n",
      "   8.35235643   2.89890955   5.33160849  -4.38378806  -8.12424135\n",
      " -11.64556659   0.31634253  -2.34664967 -10.02030104  -2.5818692\n",
      "  -2.48316124   5.68544575  -5.97700537  -4.41587345  10.77167518\n",
      "   1.76656937  -2.25554811   0.24767072  -2.59536067  11.03796736\n",
      "  -0.92903477  -0.7450439  -18.62116719  -9.57211568  -8.56873411\n",
      "   4.08082005   9.84953605  -1.36133843  -2.34220794   5.67666708\n",
      "  -9.38795828  -3.92934691   0.39333261   4.75120157  -5.52714369\n",
      "  -2.07299997   0.19944627   6.3653882    1.30544288   6.57602173\n",
      " -14.10514504   1.1742241   -8.09356946   6.46893964  -1.38463781\n",
      " -11.83087252  -9.67430994   0.21023881  -2.52051435  -0.39856388\n",
      "   0.41856211 -10.59124937   1.36979499  -6.74677092  -0.77452559\n",
      "   4.55123585  10.02833796   7.45052414  -8.48793119  -1.44985332\n",
      "   4.60431391  -4.53031711  -7.30165161  -6.77461068  -0.13262226\n",
      "   3.44661791   2.62490962 -14.77770184   8.43614201   1.73040108\n",
      "  -4.882564     3.82479619   1.57026571  -8.85000346   2.1248864\n",
      " -12.78823964  11.17067446  -5.53935766   5.15847174 -17.79576196\n",
      "  -2.93355879 -14.89379663  -6.45574941  -4.20500501  -6.61764525\n",
      "  -7.67326417  -7.86686608  -0.6927365   -4.19923032   3.46745693\n",
      "   1.45322454  -9.11307901  12.78172499   8.09643538  -4.30043728\n",
      "  -0.1612997   -4.47019351  -0.65583483  -6.83227788   3.14112563\n",
      "  17.2522613    5.30523303  -9.65530896  -5.50820169  -2.03625748\n",
      "  -7.46933886  -6.08066049 -10.91849448   4.69872883   9.24286705\n",
      "  -1.5771136   -1.04205802 -14.03620962  -5.31455288  -5.25658425\n",
      "  -1.95430406  -6.74458667  -9.91598169  -3.06751208   1.13477312\n",
      "  -6.65874478   9.87250804   4.43377515 -12.19727628 -12.00743393\n",
      " -11.08126966   2.01238976  -5.5266816   -4.63667047  -3.92255629\n",
      "  -3.72715452  -2.46989685  -6.82246982   0.32879257  -9.12938966\n",
      "  -2.43264692   3.51490165   7.31739695   3.32381468   4.96947397\n",
      "  -2.9806111   -8.97336882  -2.05500065   1.69104262  -7.63067117\n",
      "   4.45105982   0.85148449  -0.32065912 -11.27237502   1.75333295\n",
      "   5.47542754  -3.88415346   5.96291949   2.50899269   0.66677526\n",
      "  -0.03882359  -4.4817761    1.55915704   4.18091905   1.1254736\n",
      " -13.52384257   8.21403085  -8.63682775   2.57919911   3.00125744\n",
      "  -7.29006354   2.51725947  -1.00663105   0.60701193   3.86838498\n",
      "  -9.13495243 -12.77724038   1.49878104  -8.038605    -5.6021163\n",
      "  -5.73566379  -0.23947617  -4.2963236   -4.73848437   6.3148169\n",
      "   7.91313104   2.17461523 -11.40607068  -9.39314758   1.84354337\n",
      " -11.86182054  -0.13234113  -6.61047869  -1.68865303  -1.20937773\n",
      "  -0.9581467  -10.68793502   6.02181372  -4.64788379   0.64696971\n",
      "  -0.50069337  -1.61322781   3.08092872  -4.61705562   5.15473665\n",
      "  -4.44475359  -1.63134634   2.03306658  -2.98314153  -6.78031678\n",
      "  -5.6029698    0.35612251  -4.2722141    3.47533636  -1.30289021\n",
      "  -4.58764927   3.26953834  -9.75444323   2.72173721 -11.50474813\n",
      "   1.65639119  -0.26544364   1.36928695  -2.32178636  -2.79006598\n",
      "  -4.47716313  -3.01264384  -2.35403228   1.48823963   2.34760001\n",
      "  -3.26014943 -13.2954505   -6.3729627   -8.21577883  -8.74013152\n",
      "   3.83775464  -4.58771507  -8.70149642 -17.4073743    0.43047917\n",
      "  -0.4210846    1.24952041 -11.79380937  -0.51004691  -3.9959327\n",
      "   2.13235544   4.80813102   4.19838629  -4.643443    -0.18471492\n",
      "   8.83315336   4.84899408  -5.44672882 -12.82022082  -5.53191531\n",
      "  -9.34378608  -0.79353578  -5.77726478   9.9827944   -5.67977269\n",
      "  -8.13605199   5.85895764  -2.52307267 -11.60200607  -3.22112584\n",
      "   1.89768471 -15.00743024  -8.21271138  -7.73385527  -2.07233184\n",
      "   3.55836787 -12.3013599   -2.87796941  -3.48743003   1.40948842\n",
      "  -0.46030088   5.97989181   4.22592014  -2.963136     9.58487717\n",
      "   1.30039678  -2.79435196  -0.62895462  -4.75345647   3.04552324\n",
      "  -8.22177211 -13.15550777   2.27499834  -6.15892041  -0.06657493\n",
      "  -2.34769719  -5.50392365   3.35670165  -8.13385532  -3.0250002\n",
      "  -2.94931076   0.4354665   -2.1825322    0.98009253   0.89569449\n",
      "   6.36645765  -4.42970377  -6.56107785   7.79210916  -5.82341304\n",
      "  -8.60260236   1.22744652  -8.08615656   1.3599606   -3.60841297\n",
      " -11.01719012  -5.92442703  -2.29283362   5.38466948   3.37928626\n",
      "  -3.16067486  -6.67508484  -2.53985922  -0.9152538    2.17361009\n",
      "  10.71602113  -9.57197194  -8.71973494  -6.13245441   2.99733143\n",
      "  -7.13960001  -4.49603382  -0.65339837   0.02945266  -4.07425796\n",
      "   1.03093325   6.87389858  -9.8804798   -1.03167612   1.38132939\n",
      "  -9.327071     4.57641106  -3.28184739   0.29842646   8.784477\n",
      "  -3.55436326   1.40349212  -7.56426155  -4.01857476  -1.75581793\n",
      "   5.60615273  -3.02257867 -11.11408865   3.06322284  -5.32877849\n",
      "  -1.20549209   2.8962412    5.51473598  -4.1723595    7.03821937\n",
      "   4.77354878  -2.32850031  -8.5390996   -7.25131338   9.48032818\n",
      "  -5.91535132  -2.64474558   6.03757102   1.88789696   1.59364903\n",
      "   2.44011018  -7.90020608  -7.58797417  -4.85913987 -12.96129788\n",
      "  -2.7824903   -7.62023987  -5.27809376  -7.67644042  -1.98375235\n",
      "  -2.83286504   2.2230898   -6.50442243   1.19403408  -8.98653068\n",
      "   7.85863651  -8.83266602  13.14744391 -13.15623967  -7.24676213\n",
      "   1.91189813   3.41807402   3.74957041   0.47929902  -3.15053139\n",
      "  -2.41795895  -5.9508983   -7.13579784  -5.44322341  -5.60517127\n",
      "  -0.80225168  -3.27467269   3.66571185  -0.19400709  -1.39172778\n",
      "   7.56941238   4.84193376 -11.46038823  -3.62270157  -1.59665544\n",
      "   9.13550065   8.33013444  -9.48513684 -11.82609569   9.51585678\n",
      "   4.80030253   1.31211072  -9.35655588  -0.91023552   1.79322696\n",
      "   6.27811984   5.53870393  -6.71266823  -3.70958861   5.04784753\n",
      "  -5.50539957 -12.13440183  -8.24706845   1.07037472   1.51657527\n",
      "  -8.68404716 -11.50385605  -5.88521446   3.37245925   9.50460152\n",
      "   2.14492097  -5.31055139 -14.68568359   0.05439902   0.17622366\n",
      "   3.55952047  -2.65458051 -10.56736988   5.40759174  -6.38909842\n",
      "  -6.08472615   8.54885291   4.1278032   -8.79583481  -3.16601862\n",
      "  -1.54993797   1.85026275  -5.7495157   -5.39426181   3.94338062\n",
      "   0.82195971  -1.09241805   1.9499092   -7.43305625  -2.67415613\n",
      "   6.20127538   0.48525329   4.52030615  -7.20237634   1.35065612\n",
      "  -1.26680601   5.68570498   2.84892795  11.71843535   1.20429857\n",
      "   5.01990017   4.64948185   5.99467555   0.14520158  -0.7910674\n",
      "   2.20534823   4.14102934  -4.79796848  -5.16801696  -5.35592816\n",
      "  -9.57819293  10.33075695 -10.56596914   6.71809927  -0.18402551\n",
      "  -7.24292468  -1.90322121  -2.36919194   6.33767738   1.50160628\n",
      "  -3.34473326   2.57534906 -13.07951991   1.16595755   4.41942372\n",
      "   0.19820018   8.82726693   0.93700983  -4.97090348   9.64345668\n",
      "  -1.02681016  -4.01507573 -10.63004171  -8.58876162  -1.21120917\n",
      "   1.18013348   0.73098513  -6.32920068   6.51667568  -5.00934388\n",
      "  -9.10274207   2.36823005   9.52737644  -7.08518244  -8.88415029\n",
      "   8.11414065  -7.83542863  -6.1464196   -6.30858047   0.40833983\n",
      "  -1.96255654   0.17585016 -14.60015459   3.47198898   4.08233155\n",
      " -16.94314822  -3.48929557   1.42245465  -5.69787039  -8.4595063\n",
      "   8.50343261  -1.94280617   6.36424956  -0.94726445   0.78899289\n",
      "  -7.9507955   -6.44891197   3.55186474 -10.52591301  -1.85882324\n",
      "  -6.07323057 -13.65461148   1.01864131   4.48415635   0.15926512\n",
      " -14.82720419   2.15541752  -3.38654168  -2.3254581    1.75844081\n",
      "  -5.84423887   2.09167194  -1.95745221  -3.13680274   6.81865009\n",
      "   5.49464066   3.47091655   9.46189934   5.59997047   1.7407659\n",
      "  -2.17691036  -1.53748875 -18.84033294   0.89952033  -6.86490744\n",
      "  -9.38631502   0.3284447   -5.50630238  -1.28398512   1.41679669\n",
      "  -6.07757955  -4.14665585   4.12032063  -1.41428651  -6.3008526\n",
      " -16.44165251  -8.65522675   0.44477327  -3.9374878   -9.13129066\n",
      "  -4.68286328   4.72463927  -1.09030861  -7.17629921  -0.9224449\n",
      "   3.56742993 -13.1240928   -6.20338475 -12.76061521   0.14330268\n",
      "  -9.58600846  -9.93678596  -5.42879623  -2.60487069 -11.06069172\n",
      "  -8.40616129  -4.86456437  -4.58066454 -10.90798792   3.76792861\n",
      "  -9.65622326   5.03004524  -9.50702432   2.86470782  -9.0978027\n",
      "  -7.90279939   5.42319565  -0.10830738 -13.02892067  -2.50312174\n",
      "  -2.08851727   4.19915577   5.4305114   13.3538289   -4.32240953\n",
      "   0.85047626  -7.17636435   8.88069019   1.23084907   4.16159505\n",
      "  -0.51819289  -8.92597644  -2.63514227  -7.3695425    3.67235912\n",
      "   6.21111707 -20.09636307  -0.12835788   3.43001882  -4.33641815\n",
      "  -1.97268094   6.38293743   1.87296368  -9.37066699  -2.30307639\n",
      "  -3.02740337 -15.51790605   6.13617392   1.55564521  -2.02241113\n",
      "  -1.11061358  -4.83450778 -12.66137783  -4.00877317  -2.50213679\n",
      " -10.57009632   4.28926028   2.5110496    2.92854004  -6.77479144\n",
      "  -4.07544393  -4.56035785 -11.9068624    4.15011678  -9.22998296\n",
      "   8.94352998  -5.54679186   1.25498207 -17.63562671 -16.61006998\n",
      "  -8.73652171  -4.26248815 -15.27265316   4.15899006   3.96220811\n",
      "   3.49988807   2.3091113    8.73420108   2.73182589  -2.47008841\n",
      "  -7.19245052  -4.87774371  -4.10397092  -5.37775335  -8.9059827\n",
      "  -2.63111735   1.51493159   6.33874883  -1.75678042 -11.90930007\n",
      "  -0.35235498  -6.91078277  -3.76394408  -3.55098046 -10.1435166\n",
      "   7.02077733  -3.04230013 -18.94661503   3.33112009  -6.29587303\n",
      " -13.60826127   3.2467289    3.37547349  -4.95366187  -2.63181979\n",
      "  -6.60065229   0.1816146   -1.29364032  -3.11405994   2.56851268\n",
      "   3.56446614   2.50967839 -16.66028412  -1.68800022   1.34722909\n",
      "  -1.11766091 -12.00405212  -0.57825622   0.03784001  -1.28428813\n",
      "  -2.48002541 -11.80283482   8.84724234  -1.11306021   3.11489659\n",
      " -11.21564453   1.53239392   3.11184211  -5.42520577 -16.38613589\n",
      "  -1.66643428   3.42045315  -6.14920605  -2.27102419  11.96898921\n",
      "  -4.87923974  -5.49049832  -2.98419585  -0.80366003   0.8347331\n",
      "  -5.62170173  -7.70108628   2.81759414   9.08043143  -7.73523328\n",
      "  -9.54488724   0.89389497  -2.76466449 -15.79875483  -0.76453511\n",
      "   5.56644629   0.4331935  -10.47144742   2.6885042    6.62379906\n",
      "   5.36412582  -0.1871139  -15.60121949   7.82221572 -12.90371523\n",
      "   2.91052241   1.70117575  -3.13126804 -10.23278176 -11.17443579\n",
      "   2.56106813  -9.36326593 -10.7833397    0.55628806  -1.83330496\n",
      "  -2.908466     6.88932109   0.33329381   5.92509538  -1.82871915\n",
      "   5.30107523   0.68512606 -10.63761439   2.47880127  -6.10189937\n",
      "  -3.77445004  -9.75644615  -0.59737907   2.55168826  -5.15982517\n",
      "  -9.99292135  -4.49154911 -12.7859277   -1.41014566 -12.42265542\n",
      "  -4.53861162   2.71172014 -10.46256983   4.9492968   -1.78372842\n",
      "   2.83812069  -0.9467186    5.82376252  -3.84055715  -8.52322432\n",
      "   1.1899074    1.87801667 -11.56710255  -2.5415316    6.25644135\n",
      "  -9.92477416   5.63592768  -4.07147718  -5.7545895   -0.06020312\n",
      "  -5.44625576   0.44867598  -0.12854571   9.20685294   8.3061693\n",
      " -11.42322094  -0.22412676   2.79577355  -5.75141615  -3.28855489\n",
      "   5.87202255  -4.99899599  -2.59744961  -6.56479053 -12.58189373\n",
      "   0.93878068  -2.42435995   0.99755748   0.5853106    0.85474338]\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "w_true = np.array([1, 2, -1, 6, 0.3, -2])\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "X = np.random.normal(0, 1, (n, 5))\n",
    "y = np.hstack((X, np.ones((n, 1)))).dot(w_true)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём эту выборку на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И применим наш класс линейной регрессии, проверив на тесте метрики RMSE, $R ^2 _{\\text{adj}}$ и BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   2.  -1.   6.   0.3 -2. ]\n",
      "[ 1.0080246   2.03996139 -1.00628385  5.80971215  0.30052277 -2.14437161]\n",
      "148\n",
      "0.0001898405383168066\n",
      "0.9999999990551638\n",
      "26417.65598723702\n"
     ]
    }
   ],
   "source": [
    "reg = SGDLinearRegressor(StoperPoint(epsilon=1e-5, max_step=1000), alpha=1e-2, batch_size=64)\n",
    "reg = reg.fit(X_train, y_train)\n",
    "print(w_true)\n",
    "print(reg.W)\n",
    "print(reg.grad_desc.stoper.k)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(root_mean_squared_error(y_test, y_pred))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(y_test, y_pred))\n",
    "    * (len(y_test) - 1)\n",
    "    / (len(y_test) - X_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, reg.W, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насколько метрики RMSE и BIC хороши трудно сказать, пока мы не сравнили с другим алгоритмом, однако поправленный $R ^2$ говорит о том, что модель хорошо соответсвует данным.\n",
    "\n",
    "Теперь применим к тем же данным линейную регрессию из пакета `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   2.  -1.   6.   0.3 -2. ]\n",
      "5.5889822400203344e-15\n",
      "1.0\n",
      "2407.7780032670466\n"
     ]
    }
   ],
   "source": [
    "reg_lr = LinearRegression()\n",
    "reg_lr = reg_lr.fit(X_train, y_train)\n",
    "print(np.hstack((reg_lr.coef_, reg_lr.intercept_)))\n",
    "y_pred = reg_lr.predict(X_test)\n",
    "print(root_mean_squared_error(y_test, y_pred))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(y_test, y_pred))\n",
    "    * (len(y_test) - 1)\n",
    "    / (len(y_test) - X_test.shape[1] - 1)\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_simple,\n",
    "        np.hstack((reg_lr.coef_, reg_lr.intercept_)),\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты почти неотличимы от нашего подхода, но всё же лучше.\n",
    "\n",
    "### Бета-регрессия\n",
    "\n",
    "Теперь приступим к бета-регрессии, которая применяется для данных, когда таргет имеет значения от $0$ до $1$ не включительно.\n",
    "\n",
    "Сгенерируем данные для бета-регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007867008856992024 0.9999883407109841\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcPElEQVR4nO3dfXTWdf348dcGbmCxzanbWE0RzUBTM8g5b8qbnbiLMunkDYeDRtLN6JzcKYO8odSCPJ7y5EE53YmeI1F21EoMIwwpm1okJ1OkUAw9uKkRG2COwT6/P/px9Z3gzea26735eJxzneP1ud7Xtdf1brpnn13XtYIsy7IAAEhIYb4HAAB4NYECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcobme4Ce6OzsjC1btsSIESOioKAg3+MAAG9ClmWxffv2qK6ujsLC1z9HMiADZcuWLVFTU5PvMQCAHnj22Wfj3e9+9+uuGZCBMmLEiIj47xMsKSnJ8zQAwJvR1tYWNTU1uZ/jr2dABsreX+uUlJQIFAAYYN7MyzO8SBYASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSMzTfAwDAYDZq7vJ8j9Ajzyycktev7wwKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJKdbgbJgwYL44Ac/GCNGjIiKioo455xzYsOGDV3WvPLKK9HQ0BAHH3xwvPOd74xp06ZFS0tLlzWbN2+OKVOmxIEHHhgVFRXxla98JXbv3v3Wnw0AMCh0K1AeeOCBaGhoiIceeihWrlwZHR0d8ZGPfCR27tyZW3PppZfGr371q7jjjjvigQceiC1btsS5556bu33Pnj0xZcqU2LVrV/zxj3+MW2+9NZYsWRJXXXVV7z0rAGBAK8iyLOvpnV988cWoqKiIBx54ID70oQ9Fa2trHHroobF06dL45Cc/GRERTz75ZIwdOzaampri5JNPjl//+tfx0Y9+NLZs2RKVlZUREbF48eL46le/Gi+++GIUFRW94ddta2uL0tLSaG1tjZKSkp6ODwB9btTc5fkeoUeeWTil1x+zOz+/39JrUFpbWyMiory8PCIi1q5dGx0dHVFfX59bM2bMmDjssMOiqakpIiKampriuOOOy8VJRMSECROira0tHn/88f1+nfb29mhra+tyAQAGrx4HSmdnZ3zpS1+KU089Nd73vvdFRERzc3MUFRVFWVlZl7WVlZXR3NycW/N/42Tv7Xtv258FCxZEaWlp7lJTU9PTsQGAAaDHgdLQ0BB/+9vfYtmyZb05z37NmzcvWltbc5dnn322z78mAJA/Q3typzlz5sQ999wTa9asiXe/+92541VVVbFr167Ytm1bl7MoLS0tUVVVlVvzyCOPdHm8ve/y2bvm1YqLi6O4uLgnowIAA1C3zqBkWRZz5syJu+66K+6///444ogjutw+bty4OOCAA2LVqlW5Yxs2bIjNmzdHXV1dRETU1dXFY489Fi+88EJuzcqVK6OkpCSOOeaYt/JcAIBBoltnUBoaGmLp0qXxi1/8IkaMGJF7zUhpaWkMHz48SktLY9asWdHY2Bjl5eVRUlISX/ziF6Ouri5OPvnkiIj4yEc+Esccc0zMmDEjrrvuumhubo4rrrgiGhoanCUBACKim4Fy8803R0TEGWec0eX4LbfcEhdddFFERHz3u9+NwsLCmDZtWrS3t8eECRPipptuyq0dMmRI3HPPPfH5z38+6urq4h3veEfMnDkzrr766rf2TACAQeMtfQ5KvvgcFAAGCp+D8j/99jkoAAB9QaAAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACSn24GyZs2amDp1alRXV0dBQUHcfffdXW6/6KKLoqCgoMtl4sSJXdZs3bo1pk+fHiUlJVFWVhazZs2KHTt2vKUnAgAMHt0OlJ07d8YJJ5wQixYtes01EydOjOeffz53+clPftLl9unTp8fjjz8eK1eujHvuuSfWrFkTs2fP7v70AMCgNLS7d5g0aVJMmjTpddcUFxdHVVXVfm9bv359rFixIv70pz/F+PHjIyLixhtvjMmTJ8f1118f1dXV3R0JABhkuh0ob8bq1aujoqIiDjrooDjrrLPi2muvjYMPPjgiIpqamqKsrCwXJxER9fX1UVhYGA8//HB84hOf2Ofx2tvbo729PXe9ra2tL8YGIHGj5i7P9wj0k15/kezEiRPjtttui1WrVsW3v/3teOCBB2LSpEmxZ8+eiIhobm6OioqKLvcZOnRolJeXR3Nz834fc8GCBVFaWpq71NTU9PbYAEBCev0Myvnnn5/75+OOOy6OP/74OPLII2P16tVx9tln9+gx582bF42NjbnrbW1tIgUABrE+f5vx6NGj45BDDomNGzdGRERVVVW88MILXdbs3r07tm7d+pqvWykuLo6SkpIuFwBg8OrzQHnuuefiX//6V4wcOTIiIurq6mLbtm2xdu3a3Jr7778/Ojs7o7a2tq/HAQAGgG7/imfHjh25syEREZs2bYp169ZFeXl5lJeXxze+8Y2YNm1aVFVVxVNPPRWXXXZZHHXUUTFhwoSIiBg7dmxMnDgxLrnkkli8eHF0dHTEnDlz4vzzz/cOHgAgInpwBuXPf/5znHjiiXHiiSdGRERjY2OceOKJcdVVV8WQIUPir3/9a3zsYx+Lo48+OmbNmhXjxo2L3//+91FcXJx7jNtvvz3GjBkTZ599dkyePDlOO+20+P73v997zwoAGNC6fQbljDPOiCzLXvP2++677w0fo7y8PJYuXdrdLw0AvE34WzwAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHKG5nsAAPJj1Nzl+R4BXpMzKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMnxxwKB5AzEP2L3zMIp+R4BBhVnUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOd0OlDVr1sTUqVOjuro6CgoK4u677+5ye5ZlcdVVV8XIkSNj+PDhUV9fH//4xz+6rNm6dWtMnz49SkpKoqysLGbNmhU7dux4S08EABg8uv05KDt37owTTjghPv3pT8e55567z+3XXXddfO9734tbb701jjjiiLjyyitjwoQJ8cQTT8SwYcMiImL69Onx/PPPx8qVK6OjoyMuvvjimD17dixduvStPyOAPBiIn90CKet2oEyaNCkmTZq039uyLIsbbrghrrjiivj4xz8eERG33XZbVFZWxt133x3nn39+rF+/PlasWBF/+tOfYvz48RERceONN8bkyZPj+uuvj+rq6rfwdACAwaBXX4OyadOmaG5ujvr6+tyx0tLSqK2tjaampoiIaGpqirKyslycRETU19dHYWFhPPzww705DgAwQPXqR903NzdHRERlZWWX45WVlbnbmpubo6KiousQQ4dGeXl5bs2rtbe3R3t7e+56W1tbb44NACRmQLyLZ8GCBVFaWpq71NTU5HskAKAP9WqgVFVVRURES0tLl+MtLS2526qqquKFF17ocvvu3btj69atuTWvNm/evGhtbc1dnn322d4cGwBITK8GyhFHHBFVVVWxatWq3LG2trZ4+OGHo66uLiIi6urqYtu2bbF27drcmvvvvz86OzujtrZ2v49bXFwcJSUlXS4AwODV7deg7NixIzZu3Ji7vmnTpli3bl2Ul5fHYYcdFl/60pfi2muvjfe85z25txlXV1fHOeecExERY8eOjYkTJ8Yll1wSixcvjo6OjpgzZ06cf/753sEDAEREDwLlz3/+c5x55pm5642NjRERMXPmzFiyZElcdtllsXPnzpg9e3Zs27YtTjvttFixYkXuM1AiIm6//faYM2dOnH322VFYWBjTpk2L733ve73wdACAwaAgy7Is30N0V1tbW5SWlkZra6tf98Ag5EPPIP+eWTil1x+zOz+/B8S7eACAtxeBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQnKH5HgDoW6PmLs/3CADd5gwKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkJyh+R4ABpJRc5fnewSAtwVnUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjr/FQ974uzYAvBZnUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5vR4oX//616OgoKDLZcyYMbnbX3nllWhoaIiDDz443vnOd8a0adOipaWlt8cAAAawoX3xoMcee2z89re//d8XGfq/L3PppZfG8uXL44477ojS0tKYM2dOnHvuufHggw/2xShvG6PmLs/3CADQa/okUIYOHRpVVVX7HG9tbY0f/ehHsXTp0jjrrLMiIuKWW26JsWPHxkMPPRQnn3xyX4wDAAwwffIalH/84x9RXV0do0ePjunTp8fmzZsjImLt2rXR0dER9fX1ubVjxoyJww47LJqaml7z8drb26Otra3LBQAYvHo9UGpra2PJkiWxYsWKuPnmm2PTpk1x+umnx/bt26O5uTmKioqirKysy30qKyujubn5NR9zwYIFUVpamrvU1NT09tgAQEJ6/Vc8kyZNyv3z8ccfH7W1tXH44YfHz372sxg+fHiPHnPevHnR2NiYu97W1iZSAGAQ6/O3GZeVlcXRRx8dGzdujKqqqti1a1ds27aty5qWlpb9vmZlr+Li4igpKelyAQAGrz4PlB07dsRTTz0VI0eOjHHjxsUBBxwQq1atyt2+YcOG2Lx5c9TV1fX1KADAANHrv+L58pe/HFOnTo3DDz88tmzZEvPnz48hQ4bEBRdcEKWlpTFr1qxobGyM8vLyKCkpiS9+8YtRV1fnHTwAQE6vB8pzzz0XF1xwQfzrX/+KQw89NE477bR46KGH4tBDD42IiO9+97tRWFgY06ZNi/b29pgwYULcdNNNvT0GADCAFWRZluV7iO5qa2uL0tLSaG1t9XqU/88HtQHQm55ZOKXXH7M7P7/9LR4AIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOb3+UfeDgU9lBYD8cgYFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOTkNVAWLVoUo0aNimHDhkVtbW088sgj+RwHAEhE3gLlpz/9aTQ2Nsb8+fPjL3/5S5xwwgkxYcKEeOGFF/I1EgCQiLwFyne+85245JJL4uKLL45jjjkmFi9eHAceeGD8+Mc/ztdIAEAihubji+7atSvWrl0b8+bNyx0rLCyM+vr6aGpq2md9e3t7tLe35663trZGRERbW1ufzNfZ/nKfPC4ADBR98TN272NmWfaGa/MSKC+99FLs2bMnKisruxyvrKyMJ598cp/1CxYsiG984xv7HK+pqemzGQHg7az0hr577O3bt0dpaenrrslLoHTXvHnzorGxMXe9s7Mztm7dGgcffHAUFBS8pcdua2uLmpqaePbZZ6OkpOStjsrrsNf9x173H3vdf+x1/+mrvc6yLLZv3x7V1dVvuDYvgXLIIYfEkCFDoqWlpcvxlpaWqKqq2md9cXFxFBcXdzlWVlbWqzOVlJT4hu8n9rr/2Ov+Y6/7j73uP32x12905mSvvLxItqioKMaNGxerVq3KHevs7IxVq1ZFXV1dPkYCABKSt1/xNDY2xsyZM2P8+PFx0kknxQ033BA7d+6Miy++OF8jAQCJyFugnHfeefHiiy/GVVddFc3NzfH+978/VqxYsc8LZ/tacXFxzJ8/f59fIdH77HX/sdf9x173H3vdf1LY64LszbzXBwCgH/lbPABAcgQKAJAcgQIAJEegAADJeVsEyqJFi2LUqFExbNiwqK2tjUceeeR1199xxx0xZsyYGDZsWBx33HFx77339tOkA1939voHP/hBnH766XHQQQfFQQcdFPX19W/4vw3/093v672WLVsWBQUFcc455/TtgINId/d627Zt0dDQECNHjozi4uI4+uij/XfkTeruXt9www3x3ve+N4YPHx41NTVx6aWXxiuvvNJP0w5Ma9asialTp0Z1dXUUFBTE3Xff/Yb3Wb16dXzgAx+I4uLiOOqoo2LJkiV9Pmdkg9yyZcuyoqKi7Mc//nH2+OOPZ5dccklWVlaWtbS07Hf9gw8+mA0ZMiS77rrrsieeeCK74oorsgMOOCB77LHH+nnygae7e33hhRdmixYtyh599NFs/fr12UUXXZSVlpZmzz33XD9PPvB0d6/32rRpU/aud70rO/3007OPf/zj/TPsANfdvW5vb8/Gjx+fTZ48OfvDH/6Qbdq0KVu9enW2bt26fp584OnuXt9+++1ZcXFxdvvtt2ebNm3K7rvvvmzkyJHZpZde2s+TDyz33ntvdvnll2d33nlnFhHZXXfd9brrn3766ezAAw/MGhsbsyeeeCK78cYbsyFDhmQrVqzo0zkHfaCcdNJJWUNDQ+76nj17surq6mzBggX7Xf+pT30qmzJlSpdjtbW12Wc/+9k+nXMw6O5ev9ru3buzESNGZLfeemtfjTho9GSvd+/enZ1yyinZD3/4w2zmzJkC5U3q7l7ffPPN2ejRo7Ndu3b114iDRnf3uqGhITvrrLO6HGtsbMxOPfXUPp1zMHkzgXLZZZdlxx57bJdj5513XjZhwoQ+nCzLBvWveHbt2hVr166N+vr63LHCwsKor6+Ppqam/d6nqampy/qIiAkTJrzmev6rJ3v9ai+//HJ0dHREeXl5X405KPR0r6+++uqoqKiIWbNm9ceYg0JP9vqXv/xl1NXVRUNDQ1RWVsb73ve++Na3vhV79uzpr7EHpJ7s9SmnnBJr167N/Rro6aefjnvvvTcmT57cLzO/XeTr5+KA+GvGPfXSSy/Fnj179vl02srKynjyySf3e5/m5ub9rm9ubu6zOQeDnuz1q331q1+N6urqff5FoKue7PUf/vCH+NGPfhTr1q3rhwkHj57s9dNPPx33339/TJ8+Pe69997YuHFjfOELX4iOjo6YP39+f4w9IPVkry+88MJ46aWX4rTTTossy2L37t3xuc99Lr72ta/1x8hvG6/1c7GtrS3+85//xPDhw/vk6w7qMygMHAsXLoxly5bFXXfdFcOGDcv3OIPK9u3bY8aMGfGDH/wgDjnkkHyPM+h1dnZGRUVFfP/7349x48bFeeedF5dffnksXrw436MNOqtXr45vfetbcdNNN8Vf/vKXuPPOO2P58uVxzTXX5Hs0esGgPoNyyCGHxJAhQ6KlpaXL8ZaWlqiqqtrvfaqqqrq1nv/qyV7vdf3118fChQvjt7/9bRx//PF9Oeag0N29fuqpp+KZZ56JqVOn5o51dnZGRMTQoUNjw4YNceSRR/bt0ANUT76vR44cGQcccEAMGTIkd2zs2LHR3Nwcu3btiqKioj6deaDqyV5feeWVMWPGjPjMZz4TERHHHXdc7Ny5M2bPnh2XX355FBb6/+C94bV+LpaUlPTZ2ZOIQX4GpaioKMaNGxerVq3KHevs7IxVq1ZFXV3dfu9TV1fXZX1ExMqVK19zPf/Vk72OiLjuuuvimmuuiRUrVsT48eP7Y9QBr7t7PWbMmHjsscdi3bp1ucvHPvaxOPPMM2PdunVRU1PTn+MPKD35vj711FNj48aNuQiMiPj73/8eI0eOFCevoyd7/fLLL+8TIXvDMPNn5npN3n4u9ulLcBOwbNmyrLi4OFuyZEn2xBNPZLNnz87Kysqy5ubmLMuybMaMGdncuXNz6x988MFs6NCh2fXXX5+tX78+mz9/vrcZv0nd3euFCxdmRUVF2c9//vPs+eefz122b9+er6cwYHR3r1/Nu3jevO7u9ebNm7MRI0Zkc+bMyTZs2JDdc889WUVFRXbttdfm6ykMGN3d6/nz52cjRozIfvKTn2RPP/109pvf/CY78sgjs0996lP5egoDwvbt27NHH300e/TRR7OIyL7zne9kjz76aPbPf/4zy7Ismzt3bjZjxozc+r1vM/7KV76SrV+/Plu0aJG3GfeWG2+8MTvssMOyoqKi7KSTTsoeeuih3G0f/vCHs5kzZ3ZZ/7Of/Sw7+uijs6KiouzYY4/Nli9f3s8TD1zd2evDDz88i4h9LvPnz+//wQeg7n5f/18CpXu6u9d//OMfs9ra2qy4uDgbPXp09s1vfjPbvXt3P089MHVnrzs6OrKvf/3r2ZFHHpkNGzYsq6mpyb7whS9k//73v/t/8AHkd7/73X7/27t3b2fOnJl9+MMf3uc+73//+7OioqJs9OjR2S233NLncxZkmfNgAEBaBvVrUACAgUmgAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJCc/wcw+rDRIFEgbQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_beta_true = np.random.uniform(0, 1, 6)\n",
    "\n",
    "mu_true = logit_fun(np.hstack((scale(X), np.ones((n, 1)))).dot(w_beta_true))\n",
    "phi_true = 3\n",
    "\n",
    "y_beta = np.random.beta(mu_true * phi_true, (1 - mu_true) * phi_true)\n",
    "\n",
    "print(y_beta.min(), y_beta.max())\n",
    "plt.hist(y_beta, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И применим к этим данным бета-регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_beta_train, X_beta_test, y_beta_train, y_beta_test = train_test_split(\n",
    "    X, y_beta, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16823658 0.18993474 0.46071217 0.28627986 0.24746298 0.64523764]\n",
      "[0.17352431 0.17595637 0.48424675 0.28510756 0.22701474 0.60908208] 2.869650965716117\n",
      "1000\n",
      "0.23641016916129623\n",
      "-104.95906430298052\n"
     ]
    }
   ],
   "source": [
    "beta_reg_model = SGDBetaRegressor(\n",
    "    StoperPoint(1e-5, max_step=1000), alpha=1e-2, batch_size=64\n",
    ")\n",
    "beta_reg_model = beta_reg_model.fit(X_beta_train, y_beta_train)\n",
    "print(w_beta_true)\n",
    "print(beta_reg_model.W, beta_reg_model.phi)\n",
    "print(beta_reg_model.grad_desc.stoper.k)\n",
    "y_beta_pred_model = beta_reg_model.predict(X_beta_test)\n",
    "print(\n",
    "    root_mean_squared_error(\n",
    "        y_beta_test,\n",
    "        y_beta_pred_model,\n",
    "        sample_weight=(1 + beta_reg_model.phi)\n",
    "        / (y_beta_pred_model * (1 - y_beta_pred_model)),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_beta,\n",
    "        np.hstack((beta_reg_model.phi, beta_reg_model.W)),\n",
    "        X_beta_test,\n",
    "        y_beta_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              BetaModel Results                               \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                 233.37\n",
      "Model:                      BetaModel   AIC:                            -452.7\n",
      "Method:            Maximum Likelihood   BIC:                            -420.4\n",
      "Date:                Sat, 14 Dec 2024                                         \n",
      "Time:                        19:17:10                                         \n",
      "No. Observations:                 750                                         \n",
      "Df Residuals:                     743                                         \n",
      "Df Model:                           5                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.1355      0.037      3.665      0.000       0.063       0.208\n",
      "x2             0.1540      0.036      4.228      0.000       0.083       0.225\n",
      "x3             0.4875      0.038     12.900      0.000       0.413       0.562\n",
      "x4             0.3021      0.038      8.024      0.000       0.228       0.376\n",
      "x5             0.2505      0.037      6.778      0.000       0.178       0.323\n",
      "const          0.5881      0.038     15.483      0.000       0.514       0.663\n",
      "precision      2.9199      0.139     21.079      0.000       2.648       3.191\n",
      "==============================================================================\n",
      "0.23741194164462667\n",
      "-102.59441529624792\n"
     ]
    }
   ],
   "source": [
    "beta_reg_mod_model = BetaModel(\n",
    "    y_beta_train,\n",
    "    np.hstack((np.array(X_beta_train), np.ones((X_beta_train.shape[0], 1)))),\n",
    "    link_precision=Identity()\n",
    ")\n",
    "beta_reg_mod_model = beta_reg_mod_model.fit()\n",
    "print(beta_reg_mod_model.summary())\n",
    "\n",
    "y_beta_pred_mod_model = beta_reg_mod_model.predict(\n",
    "    np.hstack((np.array(X_beta_test), np.ones((X_beta_test.shape[0], 1))))\n",
    ")\n",
    "print(\n",
    "    root_mean_squared_error(\n",
    "        y_beta_test,\n",
    "        y_beta_pred_mod_model,\n",
    "        sample_weight=(1 + beta_reg_mod_model.params[-1])\n",
    "        / (y_beta_pred_mod_model * (1 - y_beta_pred_mod_model)),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_beta,\n",
    "        np.hstack(\n",
    "            (\n",
    "                beta_reg_mod_model.params[-1],\n",
    "                beta_reg_mod_model.params[:-1],\n",
    "            )\n",
    "        ),\n",
    "        X_beta_test,\n",
    "        y_beta_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм из пакета дал самую лучшую оценку.\n",
    "\n",
    "Рассмотим теперь реальные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_id</th>\n",
       "      <th>Visit_id</th>\n",
       "      <th>Species_id</th>\n",
       "      <th>Cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>6345</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>16199</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>28382</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40703</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49283</td>\n",
       "      <td>2338</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Site_id  Visit_id  Species_id  Cover\n",
       "0        4      6345        2338   0.02\n",
       "1        4     16199        2338   0.03\n",
       "2        4     28382        2338   0.03\n",
       "3        4     40703        2338   0.02\n",
       "4        4     49283        2338   0.01"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_data = pd.read_csv(\"observations.csv\", sep=\";\")\n",
    "beta_data = beta_data.drop(columns=\"Cover_class\")\n",
    "target_column = \"Cover\"\n",
    "features_columns = beta_data.drop(columns=target_column).columns\n",
    "\n",
    "beta_data[\"Cover\"] = beta_data[\"Cover\"] / 100\n",
    "\n",
    "beta_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на расперделения таргета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(beta_data[target_column], bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Близко к равномерному, хотя вполне возможно, что слева есть небольшое увеличение количества.\n",
    "\n",
    "Разделим на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_data_train, beta_data_test, target_train, target_test = train_test_split(\n",
    "    beta_data[features_columns],\n",
    "    beta_data[target_column],\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с линейной регрессии. Посмотрим, может, будет неплохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01029629  0.00390969 -0.07444774  0.14781972]\n",
      "0.19821878571462667\n",
      "0.08052169877750748\n",
      "34333.96781939073\n"
     ]
    }
   ],
   "source": [
    "reg_b = SGDLinearRegressor(StoperPoint(max_step=1000), alpha=1e-3, batch_size=64)\n",
    "reg_b = reg_b.fit(beta_data_train, target_train)\n",
    "print(reg_b.W)\n",
    "y_pred_b = reg_b.predict(beta_data_test)\n",
    "print(root_mean_squared_error(target_test, y_pred_b))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(target_test, y_pred_b))\n",
    "    * (len(target_test) - 1)\n",
    "    / (len(target_test) - beta_data_test.shape[1] - 1)\n",
    ")\n",
    "print(BIC(log_likelyhood_simple, reg_b.W, beta_data_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.17298850e-07  2.08623308e-08 -6.67397081e-05  1.77966213e-01]\n",
      "0.19704179867874827\n",
      "0.09140866936931613\n",
      "32405.112904250727\n"
     ]
    }
   ],
   "source": [
    "reg_lr_b = LinearRegression()\n",
    "reg_lr_b = reg_lr_b.fit(beta_data_train, target_train)\n",
    "print(np.hstack((reg_lr_b.coef_, reg_lr_b.intercept_)))\n",
    "y_pred_lr_b = reg_lr_b.predict(beta_data_test)\n",
    "print(root_mean_squared_error(target_test, y_pred_lr_b))\n",
    "print(\n",
    "    1\n",
    "    - (1 - r2_score(target_test, y_pred_lr_b))\n",
    "    * (len(target_test) - 1)\n",
    "    / (len(target_test) - beta_data_test.shape[1] - 1)\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_simple,\n",
    "        np.hstack((reg_lr_b.coef_, reg_lr_b.intercept_)),\n",
    "        beta_data_test,\n",
    "        target_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по отрицательному значению поправленного $R ^2$ линейная модель здесь не очень хороша.\n",
    "\n",
    "Применим нашу бета-регрессию к данным и посмотрим на весовой RMSE и BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02943923 -0.02372326 -0.23218236 -1.73620955]\n",
      "3.205425959739417\n",
      "0.19112065352431074\n",
      "-3449.8922908760874\n"
     ]
    }
   ],
   "source": [
    "beta_reg = SGDBetaRegressor(StoperPoint(1e-5, max_step=1000), alpha=1e-2, batch_size=100)\n",
    "beta_reg = beta_reg.fit(beta_data_train, target_train)\n",
    "print(beta_reg.W)\n",
    "print(beta_reg.phi)\n",
    "y_pred_beta = beta_reg.predict(beta_data_test)\n",
    "print(\n",
    "    root_mean_squared_error(\n",
    "        target_test,\n",
    "        y_pred_beta,\n",
    "        sample_weight=(1 + beta_reg.phi) / (y_pred_beta * (1 - y_pred_beta)),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_beta,\n",
    "        np.hstack((beta_reg.phi, beta_reg.W)),\n",
    "        beta_data_test,\n",
    "        target_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по RMSE результат не очень, так как можно сказать, что мы ошибаемся в среднем на $30 \\%$, что много. А по BIC мы сможем сравнить с пакетом `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              BetaModel Results                               \n",
      "==============================================================================\n",
      "Dep. Variable:                  Cover   Log-Likelihood:                 5218.6\n",
      "Model:                      BetaModel   AIC:                        -1.043e+04\n",
      "Method:            Maximum Likelihood   BIC:                        -1.039e+04\n",
      "Date:                Sat, 14 Dec 2024                                         \n",
      "Time:                        19:17:12                                         \n",
      "No. Observations:                4909                                         \n",
      "Df Residuals:                    4904                                         \n",
      "Df Model:                           3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1          5.766e-07   3.96e-06      0.146      0.884   -7.18e-06    8.33e-06\n",
      "x2         -3.904e-08   7.12e-07     -0.055      0.956   -1.43e-06    1.36e-06\n",
      "x3            -0.0003   1.54e-05    -16.354      0.000      -0.000      -0.000\n",
      "const         -1.5266      0.040    -38.489      0.000      -1.604      -1.449\n",
      "precision      3.1657      0.068     46.833      0.000       3.033       3.298\n",
      "==============================================================================\n",
      "0.19154027411054386\n",
      "-3310.568916604621\n"
     ]
    }
   ],
   "source": [
    "beta_reg_mod = BetaModel(\n",
    "    target_train,\n",
    "    np.hstack((np.array(beta_data_train), np.ones((beta_data_train.shape[0], 1)))),\n",
    "    link_precision=Identity()\n",
    ")\n",
    "beta_reg_mod = beta_reg_mod.fit()\n",
    "print(beta_reg_mod.summary())\n",
    "\n",
    "y_pred_mod = beta_reg_mod.predict(\n",
    "    np.hstack((np.array(beta_data_test), np.ones((beta_data_test.shape[0], 1))))\n",
    ")\n",
    "print(\n",
    "    root_mean_squared_error(\n",
    "        target_test,\n",
    "        y_pred_mod,\n",
    "        sample_weight=(1 + beta_reg_mod.params[\"precision\"])\n",
    "        / (y_pred_mod * (1 - y_pred_mod)),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    BIC(\n",
    "        log_likelyhood_beta,\n",
    "        np.hstack((beta_reg_mod.params[\"precision\"], beta_reg_mod.params[:-1])),\n",
    "        beta_data_test,\n",
    "        target_test,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения RMSE алгоритмы примерно равны, но вот по BIC наша модель показала себя даже лучше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
